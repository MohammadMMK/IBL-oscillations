{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from scipy.stats import binomtest\n",
    "import warnings\n",
    "import submitit\n",
    "from imblearn.under_sampling import RandomUnderSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# updated classify_TF funtion \n",
    "\n",
    "## changes:\n",
    "### 1. saving the pickle result with higher protocol adding <protocol=pickle.HIGHEST_PROTOCOL> \n",
    "### 2. put dual='auto' in selectModel\n",
    "### 3. add 'new' to the name of final file \n",
    "\n",
    "\n",
    "\n",
    "the reason for rerunning the file was that the previous jobs apparantly  did not saved the decoding result properly so I could not load them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mne\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from scipy.stats import binomtest\n",
    "import pickle\n",
    "\n",
    "def classify_TF(eid, save_tfr=False):\n",
    "    nrepeats_undersampling = 5\n",
    "    n_kfolds = 5\n",
    "\n",
    "    from sklearn.exceptions import ConvergenceWarning\n",
    "    warnings.simplefilter('ignore', category=ConvergenceWarning)\n",
    "    print('### WARNING: convergence warning deactivated (LinearSVC used for feature selection does not converge)')\n",
    "    print(f'### PROCESSING EID: {eid}')\n",
    "\n",
    "    eids_dir = \"/mnt/data/AdaptiveControl/IBLrawdata/eid_data\"\n",
    "    pids_dir = \"/mnt/data/AdaptiveControl/IBLrawdata/pid_data\"\n",
    "    probe_filelist = glob.glob(os.path.join(f'{eids_dir}/{eid}', 'probe_*.pkl'))\n",
    "    pids = [x.split('probe_')[1].split('.pkl')[0] for x in probe_filelist]\n",
    "\n",
    "    classif_df = pd.DataFrame()\n",
    "    classif_output_path = os.path.join(f'{eids_dir}/{eid}', 'classification_screener_new.pkl')\n",
    "\n",
    "    for pid in pids:\n",
    "        pid_path = f'{pids_dir}/{pid}'\n",
    "\n",
    "        # target file\n",
    "        epoch_file = os.path.join(pid_path, f'lfp_{pid}_epoched_csd.fif')\n",
    "\n",
    "        if not os.path.exists(epoch_file):\n",
    "            print(f'epoch file expected at {epoch_file} not found, skip it...')\n",
    "            continue\n",
    "\n",
    "        tfr_dir = f'/mnt/data/AdaptiveControl/IBLrawdata/TF_data_large/{pid}'\n",
    "        tfr_file = os.path.join(tfr_dir, 'tfr_multitaper_fast.h5')\n",
    "\n",
    "        if not os.path.exists(tfr_dir):\n",
    "            os.makedirs(tfr_dir)\n",
    "            os.chmod(tfr_dir, 0o775)\n",
    "\n",
    "        if not os.path.exists(tfr_file):\n",
    "            # file based on average TFR\n",
    "            epochs = mne.read_epochs(epoch_file, preload=True)\n",
    "\n",
    "            # crude TF definition for classification\n",
    "            freqs = np.concatenate([np.arange(2, 20, 2), np.arange(20, 48, 4), np.arange(60, 200, 40)])\n",
    "            n_cycles = freqs / 3\n",
    "            time_bandwidth = 3\n",
    "\n",
    "            tfr_epoch = epochs.compute_tfr(\n",
    "                method='multitaper',\n",
    "                freqs=freqs,\n",
    "                n_cycles=n_cycles,\n",
    "                time_bandwidth=time_bandwidth,\n",
    "                tmin=-0.3, tmax=0.3,\n",
    "                decim=2,\n",
    "                n_jobs=4\n",
    "            )\n",
    "\n",
    "            if save_tfr:\n",
    "                tfr_epoch.save(tfr_file, overwrite=True)\n",
    "\n",
    "        else:\n",
    "            tfr_epoch = mne.time_frequency.read_tfrs(tfr_file)\n",
    "\n",
    "        # process metadata\n",
    "        constrast0_indices = np.where(((tfr_epoch.metadata['contrastRight'].isna()) & (tfr_epoch.metadata['contrastLeft'] == 0)) |\n",
    "                                      (tfr_epoch.metadata['contrastLeft'].isna()) & (tfr_epoch.metadata['contrastRight'] == 0))[0]\n",
    "        constrastHigh_indices = np.where(((tfr_epoch.metadata['contrastRight'].isna()) & (tfr_epoch.metadata['contrastLeft'] > 0.1)) |\n",
    "                                         (tfr_epoch.metadata['contrastLeft'].isna()) & (tfr_epoch.metadata['contrastRight'] > 0.1))[0]\n",
    "        constrastLow_indices = np.where(((tfr_epoch.metadata['contrastRight'].isna()) & (tfr_epoch.metadata['contrastLeft'] < 0.2)) |\n",
    "                                        (tfr_epoch.metadata['contrastLeft'].isna()) & (tfr_epoch.metadata['contrastRight'] < 0.2))[0]\n",
    "        constrastLeft_indices = np.where(((tfr_epoch.metadata['contrastRight'].isna()) & (tfr_epoch.metadata['contrastLeft'] > 0)))[0]\n",
    "        constrastRight_indices = np.where(((tfr_epoch.metadata['contrastLeft'].isna()) & (tfr_epoch.metadata['contrastRight'] > 0)))[0]\n",
    "        blockBalanced_indices = np.where((tfr_epoch.metadata['probabilityLeft'] == 0.5))[0]\n",
    "        blockRight_indices = np.where((tfr_epoch.metadata['probabilityLeft'] < 0.45))[0]\n",
    "        blockLeft_indices = np.where((tfr_epoch.metadata['probabilityLeft'] > 0.45))[0]\n",
    "\n",
    "        # load tfrdata in ram\n",
    "        all_tfrdata = tfr_epoch.get_data()\n",
    "        all_tfrdata = np.squeeze(all_tfrdata)\n",
    "\n",
    "        all_chnames = tfr_epoch.ch_names\n",
    "\n",
    "        for chan, ch_name in enumerate(all_chnames):\n",
    "            print(f'### PROCESSING CHANNEL: {ch_name}')\n",
    "\n",
    "            clf = Pipeline([\n",
    "                ('scaling', StandardScaler()),\n",
    "                ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", max_iter=1000, dual='auto'))),\n",
    "                ('classification', LogisticRegression())\n",
    "            ])\n",
    "\n",
    "            # build labels (i.e. y in the logistic regression / classification)\n",
    "            labels_nostim = constrast0_indices\n",
    "            labels_stim = constrastHigh_indices\n",
    "            labels_nostim_stim = np.concatenate((np.zeros((labels_nostim.shape[0], 1)), np.ones((labels_stim.shape[0], 1))), axis=0)\n",
    "            labels_left = constrastLeft_indices\n",
    "            labels_right = constrastRight_indices\n",
    "            labels_left_right = np.concatenate((np.zeros((labels_left.shape[0], 1)), np.ones((labels_right.shape[0], 1))), axis=0)\n",
    "            labels_biasleft = np.setdiff1d(blockLeft_indices, constrast0_indices)\n",
    "            labels_biasright = np.setdiff1d(blockRight_indices, constrast0_indices)\n",
    "            labels_biasleft_biasright = np.concatenate((np.zeros((labels_biasleft.shape[0], 1)), np.ones((labels_biasright.shape[0], 1))), axis=0)\n",
    "\n",
    "            # electrode loop\n",
    "            ch_tfrdata = np.squeeze(all_tfrdata[:, chan, :, :])\n",
    "\n",
    "            # first classification: nostim_stim\n",
    "            X = np.concatenate((ch_tfrdata[labels_nostim, :, :], ch_tfrdata[labels_stim, :, :]), axis=0)\n",
    "            X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])  # flatten time-frequency axes\n",
    "            y = np.ravel(labels_nostim_stim)\n",
    "            test_counts = 0\n",
    "            success_counts = 0\n",
    "            keep_counts = []\n",
    "            keep_successes = []\n",
    "\n",
    "            for rus_idx in range(nrepeats_undersampling):\n",
    "                rus = RandomUnderSampler(random_state=42)\n",
    "                X_res, y_res = rus.fit_resample(X, y)\n",
    "                skf = StratifiedKFold(n_splits=n_kfolds)\n",
    "                skf.get_n_splits(X_res, y_res)\n",
    "\n",
    "                for i, (train_index, test_index) in enumerate(skf.split(X_res, y_res)):\n",
    "                    clf.fit(X_res[train_index, :], y_res[train_index])\n",
    "                    test_counts += test_index.shape[0]\n",
    "                    success_counts += int(clf.score(X_res[test_index, :], y_res[test_index]) * test_index.shape[0])\n",
    "                keep_counts.append(test_counts)\n",
    "                keep_successes.append(success_counts)\n",
    "\n",
    "            cv_acc_nostim_stim = np.nanmean(success_counts) / np.nanmean(test_counts)\n",
    "            stats_binom = binomtest(k=int(np.nanmean(success_counts)), n=int(np.nanmean(test_counts)), p=0.5, alternative=\"greater\")\n",
    "            cv_p_nostim_stim = stats_binom.pvalue\n",
    "            print(f'Global nostim_stim accuracy: {i}: {cv_acc_nostim_stim} / pvalue={cv_p_nostim_stim}')\n",
    "\n",
    "            # second classification: left_right\n",
    "            X = np.concatenate((ch_tfrdata[labels_left, :, :], ch_tfrdata[labels_right, :, :]), axis=0)\n",
    "            X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])  # flatten time-frequency axes\n",
    "            y = np.ravel(labels_left_right)\n",
    "            test_counts = 0\n",
    "            success_counts = 0\n",
    "            keep_counts = []\n",
    "            keep_successes = []\n",
    "\n",
    "            for rus_idx in range(nrepeats_undersampling):\n",
    "                rus = RandomUnderSampler(random_state=42)\n",
    "                X_res, y_res = rus.fit_resample(X, y)\n",
    "                skf = StratifiedKFold(n_splits=n_kfolds)\n",
    "                skf.get_n_splits(X_res, y_res)\n",
    "\n",
    "                for i, (train_index, test_index) in enumerate(skf.split(X_res, y_res)):\n",
    "                    clf.fit(X_res[train_index, :], y_res[train_index])\n",
    "                    test_counts += test_index.shape[0]\n",
    "                    success_counts += int(clf.score(X_res[test_index, :], y_res[test_index]) * test_index.shape[0])\n",
    "                keep_counts.append(test_counts)\n",
    "                keep_successes.append(success_counts)\n",
    "\n",
    "            cv_acc_left_right = np.nanmean(success_counts) / np.nanmean(test_counts)\n",
    "            stats_binom = binomtest(k=int(np.nanmean(success_counts)), n=int(np.nanmean(test_counts)), p=0.5, alternative=\"greater\")\n",
    "            cv_p_left_right = stats_binom.pvalue\n",
    "            print(f'Global left_right accuracy: {i}: {cv_acc_left_right} / pvalue={cv_p_left_right}')\n",
    "\n",
    "            # third classification: biasleft_biasright\n",
    "            X = np.concatenate((ch_tfrdata[labels_biasleft, :, :], ch_tfrdata[labels_biasright, :, :]), axis=0)\n",
    "            X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])  # flatten time-frequency axes\n",
    "            y = np.ravel(labels_biasleft_biasright)\n",
    "            test_counts = 0\n",
    "            success_counts = 0\n",
    "            keep_counts = []\n",
    "            keep_successes = []\n",
    "\n",
    "            for rus_idx in range(nrepeats_undersampling):\n",
    "                rus = RandomUnderSampler(random_state=42)\n",
    "                X_res, y_res = rus.fit_resample(X, y)\n",
    "                skf = StratifiedKFold(n_splits=n_kfolds)\n",
    "                skf.get_n_splits(X_res, y_res)\n",
    "\n",
    "                for i, (train_index, test_index) in enumerate(skf.split(X_res, y_res)):\n",
    "                    clf.fit(X_res[train_index, :], y_res[train_index])\n",
    "                    test_counts += test_index.shape[0]\n",
    "                    success_counts += int(clf.score(X_res[test_index, :], y_res[test_index]) * test_index.shape[0])\n",
    "                keep_counts.append(test_counts)\n",
    "                keep_successes.append(success_counts)\n",
    "\n",
    "            cv_acc_biasleft_biasright = np.nanmean(success_counts) / np.nanmean(test_counts)\n",
    "            stats_binom = binomtest(k=int(np.nanmean(success_counts)), n=int(np.nanmean(test_counts)), p=0.5, alternative=\"greater\")\n",
    "            cv_p_biasleft_biasright = stats_binom.pvalue\n",
    "            print(f'Global biasleft_biasright accuracy: {i}: {cv_acc_biasleft_biasright} / pvalue={cv_p_biasleft_biasright}')\n",
    "\n",
    "            row_df = pd.DataFrame({\n",
    "                'nostim_stim_acc': cv_acc_nostim_stim,\n",
    "                'nostim_stim_pval': cv_p_nostim_stim,\n",
    "                'left_right_acc': cv_acc_left_right,\n",
    "                'left_right_pval': cv_p_left_right,\n",
    "                'biasleft_biasright_acc': cv_acc_biasleft_biasright,\n",
    "                'biasleft_biasright_pval': cv_p_biasleft_biasright\n",
    "            }, index=[chan])\n",
    "            row_df['pid'] = pid\n",
    "            row_df['ch_name'] = ch_name\n",
    "            row_df['ch_idx'] = chan\n",
    "            classif_df = pd.concat([classif_df, row_df], axis=0)\n",
    "\n",
    "    try:\n",
    "        with open(classif_output_path, 'wb') as f:\n",
    "            pickle.dump(classif_df, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f'Successfully saved classification data to {classif_output_path}')\n",
    "    except Exception as e:\n",
    "        print(f'Error saving classification data: {e}')\n",
    "\n",
    "    return classif_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submit the funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### all jobs submitted ###\n",
      "the kernel will now be killed (and your notebook will crash) but you can see that your jobs keep running by typing squeue in the terminal\n",
      "crucially, the content of tuto_output/ will still be updated in the background!\n"
     ]
    }
   ],
   "source": [
    "base_dir='/mnt/data/AdaptiveControl/IBLrawdata/'\n",
    "eids_dir = base_dir + '/eid_data'\n",
    "pids_dir = base_dir + '/pid_data'\n",
    "\n",
    "# list all eids of interest\n",
    "eids=os.listdir(eids_dir)\n",
    "\n",
    "local_computing=False\n",
    "\n",
    "if local_computing:\n",
    "  \n",
    "  for eid in eids:\n",
    "    \n",
    "    classif_df=classify_TF(eid)\n",
    "\n",
    "else:\n",
    "\n",
    "  # prepare executor\n",
    "  executor = submitit.AutoExecutor(folder=\"classif_logs\")\n",
    "\n",
    "  # define maxjobs to a low value to illustrate\n",
    "  maxjobs=10000\n",
    "\n",
    "  # pass parameter to the executor\n",
    "  executor.update_parameters(slurm_array_parallelism=maxjobs, mem_gb=16, timeout_min=600, slurm_partition=\"CPU\", cpus_per_task=5)\n",
    "\n",
    "  # execute the job (note the .map_array command that different from the .submit command used above)\n",
    "  jobs = executor.map_array(classify_TF, eids)  # just a list of jobs\n",
    "\n",
    "  print('### all jobs submitted ###')\n",
    "  print('the kernel will now be killed (and your notebook will crash) but you can see that your jobs keep running by typing squeue in the terminal')\n",
    "  print('crucially, the content of tuto_output/ will still be updated in the background!')\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test submit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa3432cd-62bd-40bc-bc1c-a12d53bcbdcf']\n",
      "### all jobs submitted ###\n",
      "the kernel will now be killed (and your notebook will crash) but you can see that your jobs keep running by typing squeue in the terminal\n",
      "crucially, the content of tuto_output/ will still be updated in the background!\n"
     ]
    }
   ],
   "source": [
    "eids =['aa3432cd-62bd-40bc-bc1c-a12d53bcbdcf']\n",
    "\n",
    "base_dir='/mnt/data/AdaptiveControl/IBLrawdata/'\n",
    "eids_dir = base_dir + '/eid_data'\n",
    "pids_dir = base_dir + '/pid_data'\n",
    "\n",
    "print(eids)\n",
    "local_computing=False\n",
    "\n",
    "if local_computing:\n",
    "  \n",
    "  for eid in eids:\n",
    "    \n",
    "    classif_df=classify_TF(eid)\n",
    "\n",
    "else:\n",
    "\n",
    "  # prepare executor\n",
    "  executor = submitit.AutoExecutor(folder=\"classif_logs\")\n",
    "\n",
    "  # define maxjobs to a low value to illustrate\n",
    "  maxjobs=10000\n",
    "\n",
    "  # pass parameter to the executor\n",
    "  executor.update_parameters(slurm_array_parallelism=maxjobs, mem_gb=12, timeout_min=600, slurm_partition=\"CPU\", cpus_per_task=4)\n",
    "\n",
    "  # execute the job (note the .map_array command that different from the .submit command used above)\n",
    "  jobs = executor.map_array(classify_TF, eids)  # just a list of jobs\n",
    "\n",
    "  print('### all jobs submitted ###')\n",
    "  print('the kernel will now be killed (and your notebook will crash) but you can see that your jobs keep running by typing squeue in the terminal')\n",
    "  print('crucially, the content of tuto_output/ will still be updated in the background!')\n",
    "\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_TF(eid, save_tfr=False):\n",
    "  \n",
    "  nrepeats_undersampling=5\n",
    "  n_kfolds=5\n",
    "  \n",
    "  from sklearn.exceptions import ConvergenceWarning\n",
    "  warnings.simplefilter('ignore', category=ConvergenceWarning)\n",
    "  print('### WARNING: convergence warning deactivated (LinearSVC used for feature selection does not converge)')\n",
    "  print(f'### PROCESSING EID: {eid}')\n",
    "  \n",
    "  probe_filelist=glob.glob(os.path.join(f'{eids_dir}/{eid}','probe_*.pkl'))\n",
    "  pids=[x.split('probe_')[1].split('.pkl')[0] for x in probe_filelist]\n",
    "  \n",
    "  classif_df=pd.DataFrame()\n",
    "  classif_output_path=os.path.join(os.path.join(f'{eids_dir}/{eid}','classification_screener_new.pkl'))\n",
    "\n",
    "  for pid in pids:\n",
    "    \n",
    "    pid_path = f'{pids_dir}/{pid}'\n",
    "    \n",
    "    # target file\n",
    "    epoch_file = os.path.join(pid_path, f'lfp_{pid}_epoched_csd.fif')\n",
    "    \n",
    "    if not os.path.exists(epoch_file):\n",
    "      print(f'epoch file expected at {epoch_file} not found, skip it...')\n",
    "      continue\n",
    "\n",
    "    tfr_dir=f'/mnt/data/AdaptiveControl/IBLrawdata/TF_data_large/{pid}'\n",
    "    tfr_file = os.path.join(tfr_dir,'tfr_multitaper_fast.h5')\n",
    "\n",
    "    if not os.path.exists(tfr_dir):\n",
    "      os.makedirs(tfr_dir)\n",
    "      os.chmod(tfr_dir, 0o775)\n",
    "    \n",
    "    if not os.path.exists(tfr_file):\n",
    "      \n",
    "      # file based on average TFR\n",
    "      epochs = mne.read_epochs(epoch_file, preload=True)\n",
    "\n",
    "      # crude TF definition for classification\n",
    "      freqs = np.concatenate([np.arange(2, 20, 2), np.arange(20, 48, 4), np.arange(60, 200, 40)])\n",
    "      n_cycles = freqs / 3\n",
    "      time_bandwidth = 3\n",
    "\n",
    "      tfr_epoch=epochs.compute_tfr(\n",
    "              method= 'multitaper',\n",
    "              freqs= freqs,\n",
    "              n_cycles = n_cycles,\n",
    "              time_bandwidth=time_bandwidth,\n",
    "              tmin=-0.3, tmax=0.3,\n",
    "              decim=2,\n",
    "              n_jobs=4\n",
    "      )\n",
    "      \n",
    "      if save_tfr:\n",
    "        tfr_epoch.save(tfr_path, overwrite=True)\n",
    "\n",
    "    else:\n",
    "      \n",
    "      tfr_epoch=mne.time_frequency.read_tfrs(tfr_file)\n",
    "\n",
    "    ### process metadata\n",
    "    #\n",
    "    constrast0_indices=np.where(((tfr_epoch.metadata['contrastRight'].isna()) & (tfr_epoch.metadata['contrastLeft']==0)) |\n",
    "    (tfr_epoch.metadata['contrastLeft'].isna()) & (tfr_epoch.metadata['contrastRight']==0))[0]\n",
    "    #\n",
    "    constrastHigh_indices=np.where(((tfr_epoch.metadata['contrastRight'].isna()) & (tfr_epoch.metadata['contrastLeft']>0.1)) |\n",
    "    (tfr_epoch.metadata['contrastLeft'].isna()) & (tfr_epoch.metadata['contrastRight']>0.1))[0]\n",
    "    #\n",
    "    constrastLow_indices=np.where(((tfr_epoch.metadata['contrastRight'].isna()) & (tfr_epoch.metadata['contrastLeft']<0.2)) |\n",
    "    (tfr_epoch.metadata['contrastLeft'].isna()) & (tfr_epoch.metadata['contrastRight']<0.2))[0]\n",
    "    #\n",
    "    constrastLeft_indices=np.where(((tfr_epoch.metadata['contrastRight'].isna()) & (tfr_epoch.metadata['contrastLeft']>0)))[0]\n",
    "    constrastRight_indices=np.where(((tfr_epoch.metadata['contrastLeft'].isna()) & (tfr_epoch.metadata['contrastRight']>0)))[0]\n",
    "    #\n",
    "    blockBalanced_indices=np.where((tfr_epoch.metadata['probabilityLeft']==0.5))[0]\n",
    "    blockRight_indices=np.where((tfr_epoch.metadata['probabilityLeft']<0.45))[0]\n",
    "    blockLeft_indices=np.where((tfr_epoch.metadata['probabilityLeft']>0.45))[0]\n",
    "\n",
    "  \n",
    "    # load tfrdata in ram\n",
    "    all_tfrdata=tfr_epoch.get_data()\n",
    "    all_tfrdata=np.squeeze(all_tfrdata)\n",
    "    \n",
    "    all_chnames= tfr_epoch.ch_names\n",
    "        \n",
    "    for chan, ch_name in enumerate(all_chnames):\n",
    "      print(f'### PROCESSING CHANNEL: {ch_name}')\n",
    "      \n",
    "      clf = Pipeline([\n",
    "        ('scaling', StandardScaler()),\n",
    "        ('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", max_iter=1000))),\n",
    "        ('classification', LogisticRegression())\n",
    "      ])\n",
    "\n",
    "      # build labels (i.e. y in the logistic regression / classification)\n",
    "      labels_nostim=constrast0_indices\n",
    "      labels_stim=constrastHigh_indices\n",
    "      labels_nostim_stim = np.concatenate((np.zeros((labels_nostim.shape[0],1)), np.ones((labels_stim.shape[0],1))),axis=0)\n",
    "      #\n",
    "      labels_left=constrastLeft_indices\n",
    "      labels_right=constrastRight_indices\n",
    "      labels_left_right = np.concatenate((np.zeros((labels_left.shape[0],1)), np.ones((labels_right.shape[0],1))),axis=0)\n",
    "      #\n",
    "      labels_biasleft=np.setdiff1d(blockLeft_indices,constrast0_indices)\n",
    "      labels_biasright=np.setdiff1d(blockRight_indices,constrast0_indices)\n",
    "      labels_biasleft_biasright = np.concatenate((np.zeros((labels_biasleft.shape[0],1)), np.ones((labels_biasright.shape[0],1))),axis=0)\n",
    "\n",
    "      #### electrode loop\n",
    "      # \n",
    "      ch_tfrdata=np.squeeze(all_tfrdata[:,chan,:,:])\n",
    "\n",
    "      #### first classification: nostim_stim\n",
    "      # build X and y\n",
    "      X=np.concatenate((ch_tfrdata[labels_nostim,:,:],ch_tfrdata[labels_stim,:,:]),axis=0)\n",
    "      X=X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # flatten time-frequency axes\n",
    "      # format labels\n",
    "      y=np.ravel(labels_nostim_stim)\n",
    "      # classify nostim_stim \n",
    "      test_counts=0\n",
    "      success_counts=0\n",
    "      keep_counts=[]\n",
    "      keep_successes=[]\n",
    "      for rus_idx in range(nrepeats_undersampling):\n",
    "        \n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_res, y_res = rus.fit_resample(X, y)\n",
    "        skf = StratifiedKFold(n_splits=n_kfolds)\n",
    "        skf.get_n_splits(X_res, y_res)\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(skf.split(X_res, y_res)):\n",
    "          # fit\n",
    "          clf.fit(X_res[train_index,:], y_res[train_index])\n",
    "          test_counts=test_counts+test_index.shape[0]\n",
    "          success_counts=success_counts+int(clf.score(X_res[test_index,:],y_res[test_index])*test_index.shape[0])\n",
    "        #\n",
    "        keep_counts.append(test_counts)\n",
    "        keep_successes.append(success_counts)\n",
    "      \n",
    "      cv_acc_nostim_stim=np.nanmean(success_counts)/np.nanmean(test_counts)\n",
    "      stats_binom=binomtest(k=int(np.nanmean(success_counts)), n=int(np.nanmean(test_counts)), p=0.5, alternative=\"greater\")\n",
    "      cv_p_nostim_stim=stats_binom.pvalue\n",
    "      print(f'Global nostim_stim accuracy: {i}: {cv_acc_nostim_stim} / pvalue={cv_p_nostim_stim}')\n",
    "\n",
    "      #### second classification: left_right\n",
    "      # build X and y\n",
    "      X=np.concatenate((ch_tfrdata[labels_left,:,:],ch_tfrdata[labels_right,:,:]),axis=0)\n",
    "      X=X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # flatten time-frequency axes\n",
    "      # format labels\n",
    "      y=np.ravel(labels_left_right)\n",
    "      # classify nostim_stim \n",
    "      test_counts=0\n",
    "      success_counts=0\n",
    "      keep_counts=[]\n",
    "      keep_successes=[]\n",
    "      for rus_idx in range(nrepeats_undersampling):\n",
    "        \n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_res, y_res = rus.fit_resample(X, y)\n",
    "        skf = StratifiedKFold(n_splits=n_kfolds)\n",
    "        skf.get_n_splits(X_res, y_res)\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(skf.split(X_res, y_res)):\n",
    "          # fit\n",
    "          clf.fit(X_res[train_index,:], y_res[train_index])\n",
    "          test_counts=test_counts+test_index.shape[0]\n",
    "          success_counts=success_counts+int(clf.score(X_res[test_index,:],y_res[test_index])*test_index.shape[0])\n",
    "        #\n",
    "        keep_counts.append(test_counts)\n",
    "        keep_successes.append(success_counts)\n",
    "        \n",
    "      cv_acc_left_right=np.nanmean(success_counts)/np.nanmean(test_counts)\n",
    "      stats_binom=binomtest(k=int(np.nanmean(success_counts)), n=int(np.nanmean(test_counts)), p=0.5, alternative=\"greater\")\n",
    "      cv_p_left_right=stats_binom.pvalue\n",
    "      print(f'Global left_right accuracy: {i}: {cv_acc_left_right} / pvalue={cv_p_left_right}')\n",
    "        \n",
    "      #### third classification: biasleft_biasright\n",
    "      # build X and y\n",
    "      X=np.concatenate((ch_tfrdata[labels_biasleft,:,:],ch_tfrdata[labels_biasright,:,:]),axis=0)\n",
    "      X=X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # flatten time-frequency axes\n",
    "      # format labels\n",
    "      y=np.ravel(labels_biasleft_biasright)\n",
    "      # classify nostim_stim \n",
    "      test_counts=0\n",
    "      success_counts=0\n",
    "      keep_counts=[]\n",
    "      keep_successes=[]\n",
    "      for rus_idx in range(nrepeats_undersampling):\n",
    "        \n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X_res, y_res = rus.fit_resample(X, y)\n",
    "        skf = StratifiedKFold(n_splits=n_kfolds)\n",
    "        skf.get_n_splits(X_res, y_res)\n",
    "        \n",
    "        for i, (train_index, test_index) in enumerate(skf.split(X_res, y_res)):\n",
    "          # fit\n",
    "          clf.fit(X_res[train_index,:], y_res[train_index])\n",
    "          test_counts=test_counts+test_index.shape[0]\n",
    "          success_counts=success_counts+int(clf.score(X_res[test_index,:],y_res[test_index])*test_index.shape[0])\n",
    "        #\n",
    "        keep_counts.append(test_counts)\n",
    "        keep_successes.append(success_counts)\n",
    "      \n",
    "      cv_acc_biasleft_biasright=np.nanmean(success_counts)/np.nanmean(test_counts)\n",
    "      stats_binom=binomtest(k=int(np.nanmean(success_counts)), n=int(np.nanmean(test_counts)), p=0.5, alternative=\"greater\")\n",
    "      cv_p_biasleft_biasright=stats_binom.pvalue\n",
    "      print(f'Global biasleft_biasright accuracy: {i}: {cv_acc_biasleft_biasright} / pvalue={cv_p_biasleft_biasright}')\n",
    "      \n",
    "      row_df=pd.DataFrame({\n",
    "        'nostim_stim_acc': cv_acc_nostim_stim,\n",
    "        'nostim_stim_pval': cv_p_nostim_stim,\n",
    "        'left_right_acc': cv_acc_left_right,\n",
    "        'left_right_pval': cv_p_left_right,\n",
    "        'biasleft_biasright_acc': cv_acc_biasleft_biasright, \n",
    "        'biasleft_biasright_pval': cv_p_biasleft_biasright}, index=[chan])\n",
    "      row_df['pid']=pid\n",
    "      row_df['ch_name']=ch_name\n",
    "      row_df['ch_idx']=chan\n",
    "      classif_df=pd.concat([classif_df,row_df],axis=0)\n",
    "      \n",
    "  classif_df.to_pickle(classif_output_path)\n",
    "  \n",
    "  return  classif_df         \n",
    "  \n",
    "      \n",
    "    \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
