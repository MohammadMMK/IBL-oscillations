{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# level of analysis : inter subjects (group level)\n",
    "### this notebook do group level analysis on clean channels that shoed significant p value for decoding of each condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import os \n",
    "import numpy as np\n",
    "import submitit\n",
    "import pandas as pd \n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "from mne.stats import permutation_cluster_test\n",
    "import h5py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combine data for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_tfr(pid, ch_name, version ):\n",
    "    \"\"\"\n",
    "    Load time-frequency representation (TFR) data using MNE.\n",
    "\n",
    "    Parameters:\n",
    "    pid (str): Participant ID.\n",
    "    ch_name (str): Channel name.\n",
    "\n",
    "    Returns:\n",
    "    epochTFR (mne.time_frequency.EpochsTFR or None): The loaded TFR data. Returns None if the file is not found.\n",
    "    \"\"\"\n",
    "    ch_name = ch_name.replace('/', '&')\n",
    "    tfr_path = os.path.join(base_TF_path, pid, f'powerLF_{version}_{ch_name}.h5')\n",
    "    if os.path.isfile(tfr_path):\n",
    "        epochTFR = mne.time_frequency.read_tfrs(tfr_path)\n",
    "        return epochTFR\n",
    "    else:\n",
    "        print(f'TFR file not found for {pid} - {ch_name}')\n",
    "        return None\n",
    "\n",
    "\n",
    "def compute_diff_average_tfr(epochTFR, condition1_trial, condition2_trial):\n",
    "    \"\"\"\n",
    "    Compute the difference in average TFR between two conditions.\n",
    "\n",
    "    Parameters:\n",
    "    epochTFR (mne.time_frequency.EpochsTFR): The TFR data.\n",
    "    condition1_trial (list of int): Indices of trials for condition 1.\n",
    "    condition2_trial (list of int): Indices of trials for condition 2.\n",
    "\n",
    "    Returns:\n",
    "    diff_data (numpy.ndarray): The difference in average TFR between the two conditions.\n",
    "    c1_data (numpy.ndarray): The log-transformed TFR data for condition 1.\n",
    "    c2_data (numpy.ndarray): The log-transformed TFR data for condition 2.\n",
    "    \"\"\"\n",
    "    c1_data = np.log10(epochTFR.data[condition1_trial].squeeze())\n",
    "    c2_data = np.log10(epochTFR.data[condition2_trial].squeeze())\n",
    "    c1_data_av = np.mean(c1_data, axis=0)\n",
    "    c2_data_av = np.mean(c2_data, axis=0)\n",
    "    diff_data = c1_data_av - c2_data_av\n",
    "    return c1_data_av, c2_data_av, diff_data\n",
    "\n",
    "\n",
    "def combine_layer(row, version ='csd' ):\n",
    "\n",
    "    \n",
    "    pid = row['pid']\n",
    "    layer_names=['layer1','layer2','layer4','layer5','layer6']\n",
    "\n",
    "    base_TF_path = '/mnt/data/AdaptiveControl/IBLrawdata/TF_data'\n",
    "    path_epoch_quality = f'/mnt/data/AdaptiveControl/IBLrawdata/TF_data/{pid}/epoch_quality_{version}.csv'\n",
    "\n",
    "    quality_data = pd.read_csv(path_epoch_quality)\n",
    "\n",
    "    bad_trials = quality_data[(quality_data['skewness'] > 1.5) | (quality_data['max_power'] > 500) ]['epoch'].values\n",
    "    bad_trials = list(set(bad_trials))\n",
    "    for condition, signif_channel_col in {\n",
    "        'Stim_NoStim': 'signif_channel_StimNoStim',\n",
    "        'Right_Left': 'signif_channel_RightLeft',\n",
    "        'BiasRight_BiasLeft': 'signif_channel_BiasLeftBiasRight'\n",
    "    }.items():\n",
    "        signif_channels = row[signif_channel_col]\n",
    "        if pd.isna(signif_channels) or not signif_channels:\n",
    "            continue\n",
    "        if isinstance(signif_channels, str):\n",
    "            signif_channels = eval(signif_channels)\n",
    "            \n",
    "        layer_data1 = {}\n",
    "        layer_data2 = {}\n",
    "        for item in signif_channels:\n",
    "            \n",
    "            # Ensure each item is a pair (ch_name, acc_value)\n",
    "            if not isinstance(item, (list, tuple)) or len(item) != 2:\n",
    "                print(f'Unexpected item format: {item} in {pid} - {condition}')\n",
    "                return\n",
    "            \n",
    "            ch_name, acc_value = item\n",
    "            layer_number = int(ch_name[4])  # Extracting the layer number\n",
    "            print(f' {ch_name} - acc = {acc_value}')\n",
    "            \n",
    "            epochTFR = load_tfr(pid, ch_name, version = version)\n",
    "            if epochTFR is None:\n",
    "                continue\n",
    "            \n",
    "            meta = epochTFR.metadata.reset_index()\n",
    "\n",
    "            \n",
    "            if condition == 'Stim_NoStim':\n",
    "                condition1_trial = meta.index[(meta['contrastLeft'] == 1) | (meta['contrastRight'] == 1)].tolist()\n",
    "                condition2_trial = meta.index[(meta['contrastLeft'] < 0.1) | (meta['contrastRight'] < 0.1)].tolist()\n",
    "            elif condition == 'Right_Left':\n",
    "                condition1_trial = meta.index[meta['contrastRight'] == 1].tolist()\n",
    "                condition2_trial = meta.index[meta['contrastLeft'] == 1].tolist()\n",
    "            elif condition == 'BiasRight_BiasLeft':\n",
    "                condition1_trial = meta.index[meta['probabilityLeft'] == 0.2].tolist()\n",
    "                condition2_trial = meta.index[meta['probabilityLeft'] == 0.8].tolist()\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Exclude noisy trials\n",
    "            condition1_trial = [trial for trial in condition1_trial if trial not in bad_trials]\n",
    "            condition2_trial = [trial for trial in condition2_trial if trial not in bad_trials]\n",
    "            \n",
    "            c1_data_av, c2_data_av, diff_data = compute_diff_average_tfr(epochTFR, condition1_trial, condition2_trial)\n",
    "            print(f'Computed difference in average TFR for {pid} - {ch_name} - {condition}')\n",
    "            \n",
    "            del epochTFR  # Free up memory\n",
    "            \n",
    "            if layer_number not in layer_data1 :\n",
    "                layer_data1[layer_number] = []\n",
    "                layer_data2[layer_number] = []\n",
    "            layer_data1[layer_number].append(c1_data_av)\n",
    "            layer_data2[layer_number].append(c2_data_av)\n",
    "            \n",
    "        layer_objects1 = {}\n",
    "        for layer_number, data_list in layer_data1.items():\n",
    "            combined_data = np.stack(data_list)  # Combine data for channels in this layer\n",
    "            combined_data = np.mean(combined_data, axis=0, keepdims= True) # Average across channels\n",
    "            layer_objects1[layer_number] = combined_data\n",
    "            print(f'Layer {layer_number} data shape: {combined_data.shape}')\n",
    "        \n",
    "        layer_objects2 = {}\n",
    "        for layer_number, data_list in layer_data2.items():\n",
    "            combined_data = np.stack(data_list)\n",
    "            combined_data = np.mean(combined_data, axis=0, keepdims= True)\n",
    "            layer_objects2[layer_number] = combined_data\n",
    "            print(f'Layer {layer_number} data shape: {combined_data.shape}')\n",
    "        \n",
    "        # save layer_objects\n",
    "        save_path = os.path.join(base_TF_path, pid, f'TFR_LF_{condition}_all_{version}.h5')\n",
    "        with h5py.File(save_path, 'w') as hf:   \n",
    "            for layer_name in layer_names:\n",
    "                layer_number = int(layer_name[-1])  # Extract the layer number from the layer name\n",
    "                if layer_number in layer_objects1:\n",
    "                    dataset_name = f'{layer_name}_c1'\n",
    "                    hf.create_dataset(dataset_name, data=layer_objects1[layer_number])\n",
    "                if layer_number in layer_objects2:\n",
    "                    dataset_name = f'{layer_name}_c2'\n",
    "                    hf.create_dataset(dataset_name, data=layer_objects2[layer_number])\n",
    "                    \n",
    "        print(f'Saved combined data for {pid} - {condition} in {save_path}')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.stats import permutation_cluster_test\n",
    "import h5py\n",
    "import submitit\n",
    "\n",
    "base_eid_path = '/mnt/data/AdaptiveControl/IBLrawdata/eid_data'\n",
    "base_TF_path = '/mnt/data/AdaptiveControl/IBLrawdata/TF_data'\n",
    "path_summary_data = '/crnldata/cophy/TeamProjects/mohammad/ibl-oscillations/_analyses/_IBLworkflows/preprocessing/clean_data_with_significant_channels.csv'\n",
    "summary_data = pd.read_csv(path_summary_data)\n",
    "\n",
    "# summary_data= summary_data[11:12]\n",
    "\n",
    "executor = submitit.AutoExecutor(folder=\"logs\")\n",
    "\n",
    "# Define maxjobs to a low value to illustrate\n",
    "maxjobs = 100\n",
    "\n",
    "# Pass parameter to the executor\n",
    "executor.update_parameters(slurm_array_parallelism=maxjobs, mem_gb=16, timeout_min=600, slurm_partition=\"CPU\", cpus_per_task=1)\n",
    "\n",
    "\n",
    "# Execute the job (note the .map_array command that different from the .submit command used above)\n",
    "jobs = executor.map_array(combine_layer, [row for _, row in summary_data.iterrows()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute the average over all subjects with permutation results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def group_level_permutation(condition , layer , version , n_permutations = 1000, threshold= None , n_jobs = 5):\n",
    "\n",
    "    '''\n",
    "    \n",
    "    layer should be one of these ['layer1','layer2','layer4','layer5','layer6']\n",
    "    \n",
    "    condition should be one of these 'Stim_NoStim','Right_Left','BiasRight_BiasLeft'\n",
    "    \n",
    "    '''\n",
    "    TF_path = '/mnt/data/AdaptiveControl/IBLrawdata/TF_data/'\n",
    "    group_level_data_path = f'/mnt/data/AdaptiveControl/IBLrawdata/group_level_data/{layer}'\n",
    "    os.makedirs(group_level_data_path, exist_ok=True)\n",
    "    X1=[]\n",
    "    X2 = []\n",
    "    file_name = f'TFR_LF_{condition}_all_{version}.h5'\n",
    "    no_combine_file_pid = [] # list of pid that do not have the combine file\n",
    "    dataset_name1 = f'{layer}_c1'\n",
    "    dataset_name2 = f'{layer}_c2'\n",
    "    total_subject = 0\n",
    "    for pid in os.listdir(TF_path):\n",
    "        \n",
    "        if os.path.isfile(os.path.join(TF_path, pid, file_name)):\n",
    "            \n",
    "            with h5py.File(os.path.join(TF_path, pid, file_name), 'r') as hf:\n",
    "                try:\n",
    "                    x1 = hf[dataset_name1][:] #x shape = (1, n_freqs, n_times)\n",
    "                    x2 = hf[dataset_name2][:] #x shape = (1, n_freqs, n_times)\n",
    "                    X1.append(x1)\n",
    "                    X2.append(x2)\n",
    "                    total_subject += 1\n",
    "                except:\n",
    "                    print(f'no data for {pid} in {layer}')\n",
    "                    continue\n",
    "        else:\n",
    "            no_combine_file_pid.append(pid)\n",
    "    \n",
    "    X1stack = np.vstack(X1) # X1stack shape = (n_subjects, n_freqs, n_times) for condition one for example stim trials\n",
    "    X2stack = np.vstack(X2) # X2stack shape = (n_subjects, n_freqs, n_times) for condition two for example no stim trials\n",
    "    \n",
    "    \n",
    "    print(f'shape of X1stack: {X1stack.shape}')\n",
    "    print(f'shape of X2stack: {X2stack.shape}')\n",
    "    diff_tfr = X2stack - X1stack # shape = (n_subjects, n_freqs, n_times)\n",
    "    diff_tfr_averege = np.mean(diff_tfr, axis = 0) # shape = (n_freqs, n_times)\n",
    "    X = [X2stack, X1stack]\n",
    "    \n",
    "    \n",
    "    \n",
    "    T_obs, clusters, cluster_p_values, H0 = permutation_cluster_test(X, out_type='mask', n_permutations= n_permutations, threshold=threshold, tail=0, n_jobs= n_jobs)\n",
    "    print(f'Permutation test completed for {layer} - {condition}')\n",
    "    \n",
    "    save_path = os.path.join(group_level_data_path,  f'TFRpermut_LF_{condition}_{version}.h5')\n",
    "    with h5py.File(save_path, 'w') as hf:\n",
    "        dset = hf.create_dataset('all_subjects_averageTFR', data=diff_tfr_averege)\n",
    "        dset.attrs['description'] = f'average Difference data for the layer {layer} accross all subjects for {condition} condition. the channels included in this analysis where the one that showed significant p value in decoding result of this conditoin'\n",
    "\n",
    "        dset = hf.create_dataset('T_obs', data=T_obs)\n",
    "        dset.attrs['description'] = 'Observed test statistic'\n",
    "\n",
    "        dset = hf.create_dataset('clusters', data=np.array([c.astype(int) for c in clusters]))\n",
    "        dset.attrs['description'] = 'Clusters identified in the permutation test'\n",
    "\n",
    "        dset = hf.create_dataset('cluster_p_values', data=cluster_p_values)\n",
    "        dset.attrs['description'] = 'P-values for each cluster'\n",
    "\n",
    "        dset = hf.create_dataset('H0', data=H0)\n",
    "        dset.attrs['description'] = 'Distribution of the test statistic under the null hypothesis'\n",
    "\n",
    "        dset = hf.create_dataset('condition', data=np.string_(condition))\n",
    "        dset.attrs['description'] = 'Condition for the permutation test'\n",
    "\n",
    "\n",
    "        dset = hf.create_dataset('n_permutations', data=n_permutations)\n",
    "        dset.attrs['description'] = 'Number of permutations'\n",
    "\n",
    "        dset = hf.create_dataset('threshold', data=threshold if threshold is not None else np.nan)\n",
    "        dset.attrs['description'] = 'Threshold for the permutation test'\n",
    "\n",
    "        dset = hf.create_dataset('number_of_subject', data=total_subject)\n",
    "        dset.attrs['description'] = 'Number of subjects  included in the analysis'\n",
    "\n",
    "        dset = hf.create_dataset('version', data=np.string_(version))\n",
    "        dset.attrs['description'] = 'Version of the data'\n",
    "            \n",
    "            \n",
    "    print(f'Saved permutation results for {layer} - {condition}')\n",
    "    \n",
    "       \n",
    "    if len(no_combine_file_pid) > 0:\n",
    "        print('WARNING: No combine file for the following pids:')\n",
    "        print(no_combine_file_pid)\n",
    "        print('###########################')\n",
    "    \n",
    "                \n",
    "                \n",
    "                \n",
    "    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.stats import permutation_cluster_test\n",
    "import h5py\n",
    "import submitit\n",
    "version = 'raw'\n",
    "\n",
    "for layer in ['layer1','layer2','layer4','layer5','layer6']:\n",
    "    for condition in ['Stim_NoStim','Right_Left','BiasRight_BiasLeft']:\n",
    "        executor = submitit.AutoExecutor(folder=os.getcwd()+'/logs/')\n",
    "        executor.update_parameters(mem_gb=20, timeout_min=400, slurm_partition=\"CPU\", cpus_per_task=5, slurm_comment='group_level_permutation')\n",
    "        job = executor.submit(group_level_permutation, condition, layer, version)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'csd'\n",
    "\n",
    "for layer in ['layer1','layer2','layer4','layer5','layer6']:\n",
    "    for condition in ['Stim_NoStim','Right_Left','BiasRight_BiasLeft']:\n",
    "        executor = submitit.AutoExecutor(folder=os.getcwd()+'/logs/')\n",
    "        executor.update_parameters(mem_gb=20, timeout_min=400, slurm_partition=\"CPU\", cpus_per_task=5, slurm_comment='group_level_permutation')\n",
    "        job = executor.submit(group_level_permutation, condition, layer, version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import os \n",
    "\n",
    "def load_h5_file(file_path):\n",
    "    \"\"\"\n",
    "    Load data from an HDF5 file and print the datasets.\n",
    "\n",
    "    Parameters:\n",
    "    file_path (str): The path to the HDF5 file.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the datasets.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    with h5py.File(file_path, 'r') as hf:\n",
    "        for key in hf.keys():\n",
    "            if hf[key].shape == ():  # Check if the dataset is a scalar\n",
    "                value = hf[key][()]\n",
    "                if isinstance(value, bytes):  # Check if the value is a byte string\n",
    "                    value = value.decode('utf-8')\n",
    "                data[key] = value\n",
    "            else:\n",
    "                data[key] = hf[key][:]\n",
    "                if isinstance(data[key], np.ndarray) and data[key].dtype.type is np.bytes_:\n",
    "                    data[key] = data[key].astype(str)\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "# get time and freq array from a random TFR file that is computed for low frequency band\n",
    "randomTFRpath = '/mnt/data/AdaptiveControl/IBLrawdata/TF_data/22609d51-0a6a-425d-a3ef-eeb43a9ba350/powerLF_raw_VISp6a_381.h5'\n",
    "randomTFR = mne.time_frequency.read_tfrs(randomTFRpath)\n",
    "timearray = randomTFR[0].times\n",
    "freqarray = randomTFR[0].freqs\n",
    "# Calculate time and frequency edges\n",
    "time_edges = np.concatenate(([timearray[0] - (timearray[1] - timearray[0]) / 2],\n",
    "                                (timearray[:-1] + timearray[1:]) / 2,\n",
    "                                [timearray[-1] + (timearray[-1] - timearray[-2]) / 2]))\n",
    "freq_edges = np.concatenate(([freqarray[0] - (freqarray[1] - freqarray[0]) / 2],\n",
    "                                (freqarray[:-1] + freqarray[1:]) / 2,\n",
    "                                [freqarray[-1] + (freqarray[-1] - freqarray[-2]) / 2]))\n",
    "\n",
    "\n",
    "\n",
    "version = 'raw'\n",
    "for condition in ['Stim_NoStim', 'Right_Left', 'BiasRight_BiasLeft']:\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(10, 25), constrained_layout=True)\n",
    "    fig.suptitle(f'Condition: {condition}- {version}', fontsize=16)\n",
    "    \n",
    "    for i, layer in enumerate(['layer1', 'layer2', 'layer4', 'layer5', 'layer6']):\n",
    "        file_path = f'/mnt/data/AdaptiveControl/IBLrawdata/group_level_data/{layer}/TFRpermut_LF_{condition}_{version}.h5'\n",
    "        if not os.path.isfile(file_path):\n",
    "            \n",
    "            print(f'File not found: {file_path}')\n",
    "            continue\n",
    "\n",
    "        data = load_h5_file(file_path)\n",
    "        diff_data = data['all_subjects_averageTFR']\n",
    "        clusters = [data['clusters'][i] for i in range(len(data['clusters']))]\n",
    "        cluster_p_values = data['cluster_p_values']\n",
    "        number_of_subject = data['number_of_subject']\n",
    "        nb_clusters = len(clusters)\n",
    "        # Create a mask of significant clusters\n",
    "        significant_mask = np.zeros_like(diff_data, dtype=bool)\n",
    "        for j, cluster in enumerate(clusters):\n",
    "            if cluster_p_values[j] < 0.05:\n",
    "                significant_mask |= cluster.astype(bool)\n",
    "\n",
    "        # Plot the difference\n",
    "        ax = axs[i]\n",
    "        pcm = ax.pcolormesh(time_edges, freq_edges, diff_data, cmap='RdBu_r',\n",
    "                            vmin=-np.max(np.abs(diff_data)), vmax=np.max(np.abs(diff_data)))\n",
    "        fig.colorbar(pcm, ax=ax, label='Difference (log10)')\n",
    "        ax.set_title(f'{layer} - {number_of_subject} subjects - {nb_clusters} clusters')\n",
    "        ax.set_xlabel('Time (seconds)')\n",
    "        ax.set_ylabel('Frequency (Hz)')\n",
    "\n",
    "        # Apply shading to non-significant areas\n",
    "        non_significant_mask = ~significant_mask\n",
    "        ax.pcolormesh(time_edges, freq_edges, np.ma.masked_where(non_significant_mask == False, non_significant_mask),\n",
    "                    cmap='gray', alpha=0.3)\n",
    "\n",
    "    plt.savefig(f'/crnldata/cophy/TeamProjects/mohammad/ibl-oscillations/_analyses/_IBLworkflows/decoding/figures/{condition}_{version}_groupLevel.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "version = 'csd'\n",
    "for condition in ['Stim_NoStim', 'Right_Left', 'BiasRight_BiasLeft']:\n",
    "    fig, axs = plt.subplots(5, 1, figsize=(10, 25), constrained_layout=True)\n",
    "    fig.suptitle(f'Condition: {condition}- {version}', fontsize=16)\n",
    "    \n",
    "    for i, layer in enumerate(['layer1', 'layer2', 'layer4', 'layer5', 'layer6']):\n",
    "        file_path = f'/mnt/data/AdaptiveControl/IBLrawdata/group_level_data/{layer}/TFRpermut_LF_{condition}_{version}.h5'\n",
    "        if not os.path.isfile(file_path):\n",
    "            \n",
    "            print(f'File not found: {file_path}')\n",
    "            continue\n",
    "\n",
    "        data = load_h5_file(file_path)\n",
    "        diff_data = data['all_subjects_averageTFR']\n",
    "        clusters = [data['clusters'][i] for i in range(len(data['clusters']))]\n",
    "        cluster_p_values = data['cluster_p_values']\n",
    "        number_of_subject = data['number_of_subject']\n",
    "        nb_clusters = len(clusters)\n",
    "        # Create a mask of significant clusters\n",
    "        significant_mask = np.zeros_like(diff_data, dtype=bool)\n",
    "        for j, cluster in enumerate(clusters):\n",
    "            if cluster_p_values[j] < 0.05:\n",
    "                significant_mask |= cluster.astype(bool)\n",
    "\n",
    "        # Plot the difference\n",
    "        ax = axs[i]\n",
    "        pcm = ax.pcolormesh(time_edges, freq_edges, diff_data, cmap='RdBu_r',\n",
    "                            vmin=-np.max(np.abs(diff_data)), vmax=np.max(np.abs(diff_data)))\n",
    "        fig.colorbar(pcm, ax=ax, label='Difference (log10)')\n",
    "        ax.set_title(f'{layer} - {number_of_subject} subjects - {nb_clusters} clusters')\n",
    "        ax.set_xlabel('Time (seconds)')\n",
    "        ax.set_ylabel('Frequency (Hz)')\n",
    "\n",
    "        # Apply shading to non-significant areas\n",
    "        non_significant_mask = ~significant_mask\n",
    "        ax.pcolormesh(time_edges, freq_edges, np.ma.masked_where(non_significant_mask == False, non_significant_mask),\n",
    "                    cmap='gray', alpha=0.3)\n",
    "\n",
    "    plt.savefig(f'/crnldata/cophy/TeamProjects/mohammad/ibl-oscillations/_analyses/_IBLworkflows/decoding/figures/{condition}_{version}_groupLevel.png')\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crnlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
