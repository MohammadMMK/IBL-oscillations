{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# list all the pids in the IBL dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of probe insertions: 742\n"
     ]
    }
   ],
   "source": [
    "from one.api import ONE\n",
    "import json\n",
    "# Initialize ONE with the IBL public server\n",
    "one = ONE(base_url='https://openalyx.internationalbrainlab.org')\n",
    "# Retrieve all probe insertions\n",
    "insertions = one.alyx.rest('insertions', 'list', task_protocol='ephys', performance_gte=70, \n",
    "                dataset_qc_gte='PASS')\n",
    "\n",
    "\n",
    "pid_eid_pairs = [(insertion['id'], insertion['session']) for insertion in insertions]\n",
    "print(f\"Total number of probe insertions: {len(pid_eid_pairs)}\")\n",
    "# Save pID-eID pairs to a JSON file\n",
    "with open('pid_eid_pairs.json', 'w') as f:\n",
    "    json.dump(pid_eid_pairs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file\n",
    "import json\n",
    "with open('pid_eid_pairs.json', 'r') as f:\n",
    "    pid_eid_pairs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import submitit\n",
    "import os \n",
    "from prepare_data import prepare_data\n",
    "start = 100\n",
    "end = -1\n",
    "pid_eid_pairs_short = pid_eid_pairs[start:end]\n",
    "# prepare executor\n",
    "executor = submitit.AutoExecutor(folder=\"tuto_logs\")\n",
    "# define maxjobs to a low value to illustrate\n",
    "maxjobs=50\n",
    "# pass parameter to the executor\n",
    "executor.update_parameters(slurm_array_parallelism=maxjobs, mem_gb=10, timeout_min=300, slurm_partition=\"CPU\", cpus_per_task=1)\n",
    "# execute the job (note the .map_array command that different from the .submit command used above)\n",
    "jobs = executor.map_array(prepare_data, pid_eid_pairs_short)  # just a list of jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "successful_jobs = 0\n",
    "total_jobs = len(pid_eid_pairs_short)\n",
    "failed_jobs = 0\n",
    "data_mismatch = 0\n",
    "\n",
    "for job in jobs:\n",
    "    try:\n",
    "        result = job.result()  # This will raise an exception if the job failed\n",
    "        if result == 1:\n",
    "            data_mismatch += 1\n",
    "            continue\n",
    "        successful_jobs += 1   # Increment only if job.result() did not raise an exception\n",
    "    except Exception as e:\n",
    "        failed_jobs += 1\n",
    "        # print(f\"Job {job.job_id} failed with exception: {e}\")\n",
    "\n",
    "print(f\"jobs: {start} - {end}\")\n",
    "print(f\"Successful jobs: {successful_jobs}\")\n",
    "print(f\"Failed jobs: {failed_jobs}\")\n",
    "print(f\"Data mismatch: {data_mismatch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successful jobs: 662\n",
    "Failed jobs: 23\n",
    "Data mismatch: 56\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "base_path = '/mnt/data/AdaptiveControl/IBLrawdata/classification/preprocess_data'\n",
    "all_dataframes = []\n",
    "for pid, eid in pid_eid_pairs:\n",
    "    output_path = f'{base_path}/{pid}.pkl'\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, 'rb') as f:\n",
    "            df = pickle.load(f)\n",
    "            all_dataframes.append(df)  # Add the loaded DataFrame to the list\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "combined_df = pd.concat(all_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the combined DataFrame to a pickle file\n",
    "combined_df.to_pickle('prepared_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
