{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pickle\n",
    "pid = 'a3d13b05-bf4d-427a-a2d5-2fe050d603ec'\n",
    "eid = '03cf52f6-fba6-4743-a42e-dd1ac3072343' \n",
    "base_path = '/mnt/data/AdaptiveControl/IBLrawdata/classification/preprocess_firingRate'\n",
    "path = os.path.join(base_path, f'{pid}.pkl')\n",
    "with open(path, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "firing_rate = data['firing_rates']\n",
    "trial_info = data['trial_info'] # dictionary keys ['trial_index', 'labels', 'contrasts', 'distance_to_change', 'prob_left', 'probe_id', 'experiment_id']\n",
    "index_passive = [i for i, x in enumerate(trial_info['prob_left']) if not x >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "probability_left_filter = (trial_info['prob_left'] == 0.5) | np.isnan(trial_info['prob_left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "contrast_filter = trial_info['contrasts'] == 1\n",
    "probability_left_filter = (trial_info['prob_left'] == 0.5) | np.isnan(trial_info['prob_left'])\n",
    "trial_info_filter = contrast_filter & probability_left_filter\n",
    "# Apply filters\n",
    "trial_info_filtered_dict = {key: value[trial_info_filter] for key, value in trial_info.items()}\n",
    "trial_info_filtered = pd.DataFrame(trial_info_filtered_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "firing_rate_passive = firing_rate[index_passive]\n",
    "index_active = [i for i, x in enumerate(trial_info['prob_left']) if x >= 0]\n",
    "firing_rate_active = firing_rate[index_active]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firing_rate import *\n",
    "z_score_right, z_score_left, times, depths, ids, acronyms, ch_indexs, coordinates = right_left_firingRates_onDepths(eid, pid, depth_lim= [10, 3850])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No default revision for dataset alf/#2022-10-31#/_ibl_trials.table.pqt; using most recent\n"
     ]
    }
   ],
   "source": [
    "behavior = get_behavior(eid, modee='download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior = get_behavior(eid, mode='download')\n",
    "right_onset = behavior[behavior['contrastRight'] == 1]['stimOn_times']\n",
    "left_onset = behavior[behavior['contrastLeft'] == 1]['stimOn_times']\n",
    "\n",
    "# the index of trials\n",
    "indx_right = list(right_onset.index)\n",
    "indx_left = list(left_onset.index)\n",
    "trial_indx = np.concatenate((indx_right, indx_left))\n",
    "\n",
    "# the distance to the last change \n",
    "change_indices = behavior['probabilityLeft'].ne(behavior['probabilityLeft'].shift()).to_numpy().nonzero()[0]\n",
    "distance_to_change = np.array([0 if i in change_indices else i - change_indices[change_indices < i][-1] for i in range(len(behavior))])\n",
    "\n",
    "\n",
    "probs_left = behavior['probabilityLeft'][trial_indx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bin_spike_dataa(spike_times, spike_depths, time_bin, depth_bin, time_range, depth_range=[10, 3850]):\n",
    "    # Determine the time and depth limits\n",
    "    time_min, time_max = time_range\n",
    "    depth_min, depth_max = depth_range\n",
    "    \n",
    "    def _get_scale_and_indices(values, bin_size, limits):\n",
    "        # Calculate the scale (edges) and indices for binning\n",
    "        scale = np.arange(limits[0], limits[1] + bin_size, bin_size)\n",
    "        indices = np.clip((np.floor((values - limits[0]) / bin_size)).astype(int), 0, len(scale) - 2)\n",
    "        return scale, indices\n",
    "    \n",
    "    # Get time and depth scales and indices\n",
    "    time_scale, time_indices = _get_scale_and_indices(spike_times, time_bin, (time_min, time_max))\n",
    "    depth_scale, depth_indices = _get_scale_and_indices(spike_depths, depth_bin, (depth_min, depth_max))\n",
    "    \n",
    "    # Create 2D histogram array by aggregating with bincount on indices\n",
    "    binned_spikes = np.zeros((len(depth_scale) - 1, len(time_scale) - 1))\n",
    "    ind2d = np.ravel_multi_index((depth_indices, time_indices), dims=binned_spikes.shape)\n",
    "    binned_counts = np.bincount(ind2d, minlength=binned_spikes.size)\n",
    "    binned_spikes.flat = binned_counts  # reshape to 2D array\n",
    "    \n",
    "    # Calculate centers of each bin to match binned_spikes dimensions\n",
    "    time_centers = (time_scale[:-1] + time_scale[1:]) / 2\n",
    "    depth_centers = (depth_scale[:-1] + depth_scale[1:]) / 2\n",
    "    \n",
    "    return binned_spikes, time_centers, depth_centers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 'e4ce2e94-6fb9-4afe-acbf-6f5a3498602e'\n",
    "eid = '6ab9d98c-b1e9-4574-b8fe-b9eec88097e0' \n",
    "modee = 'download'\n",
    "import numpy as np\n",
    "from get_data import *\n",
    "######################\n",
    "# Load data\n",
    "########################\n",
    "# Load behavior\n",
    "behavior = get_behavior(eid, modee=modee)\n",
    "right_onset = behavior[behavior['contrastRight'] == 1]['stimOn_times']\n",
    "left_onset = behavior[behavior['contrastLeft'] == 1]['stimOn_times']\n",
    "stim_events = {'right': right_onset, 'left': left_onset}\n",
    "\n",
    "# the index of trials\n",
    "indx_right = list(right_onset.index)\n",
    "indx_left = list(left_onset.index)\n",
    "trial_indx = np.concatenate((indx_right, indx_left))\n",
    "\n",
    "# the distance to the last change \n",
    "change_indices = behavior['probabilityLeft'].ne(behavior['probabilityLeft'].shift()).to_numpy().nonzero()[0]\n",
    "distance_to_change = np.array([0 if i in change_indices else i - change_indices[change_indices < i][-1] for i in range(len(behavior))])\n",
    "distance_to_change = distance_to_change[trial_indx]\n",
    "# the probability of left\n",
    "probs_left = behavior['probabilityLeft'][trial_indx].reset_index(drop=True)\n",
    "\n",
    "# load channel data \n",
    "channels = get_channels(eid, pid,  modee=modee) # download the data from the IBL database or 'load' data from the local directory\n",
    "# load spikes\n",
    "spikes_datasets = get_spikes(pid, modee=modee)\n",
    "spikes = spikes_datasets['spikes']\n",
    "spike_times = spikes['times']\n",
    "spike_depths = spikes['depths']\n",
    "kp_idx = np.where(~np.isnan(spike_depths))[0] # Remove any nan depths\n",
    "spike_times = spike_times[kp_idx]\n",
    "spike_depths = spike_depths[kp_idx]\n",
    "######################\n",
    "# compute firing rate\n",
    "    ########################\n",
    "pre_stim = 0.4\n",
    "post_stim = 1\n",
    "t_bin = 0.1\n",
    "d_bin = 20\n",
    "depth_lim = [10, 3840]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iblutil.numerical import bincount2D\n",
    "# Generate time and depth arrays\n",
    "pre_stim = 0.4\n",
    "post_stim = 1\n",
    "t_bin = 0.1\n",
    "d_bin = 20\n",
    "depth_lim = [10, 3840]\n",
    "z_scores = {stim_type: [] for stim_type in stim_events.keys()}\n",
    "\n",
    "for stim_type, stim_times in stim_events.items():\n",
    "    stim_times = stim_times[~np.isnan(stim_times)]\n",
    "    for stim_on_time in stim_times:\n",
    "        interval = [stim_on_time - pre_stim, stim_on_time + post_stim]\n",
    "        idx = np.where((spike_times > interval[0]) & (spike_times < interval[1]))[0]\n",
    "        spike_times_i = spike_times[idx]\n",
    "        spike_depths_i = spike_depths[idx]\n",
    "\n",
    "        # Create binned array\n",
    "        binned_array, tim, depths = bincount2D(spike_times_i, spike_depths_i, xbin= t_bin, ybin= d_bin, xlim=interval, ylim=depth_lim)\n",
    "        \n",
    "        # Compute baseline mean and std\n",
    "        baseline_mean = np.mean(binned_array[:, :int(pre_stim/t_bin)], axis=1)\n",
    "        baseline_std = np.std(binned_array[:, :int(pre_stim/t_bin)], axis=1)\n",
    "        baseline_std[baseline_std == 0] = 1  # Avoid division by zero\n",
    "\n",
    "        # Compute Z-score of firing rates\n",
    "        z_score_firing_rate = (binned_array - baseline_mean[:, np.newaxis]) / baseline_std[:, np.newaxis]\n",
    "  \n",
    "        # Append to the respective list\n",
    "        z_scores[stim_type].append(z_score_firing_rate)\n",
    "\n",
    "z_scores = {stim_type: np.array(z_score_list) for stim_type, z_score_list in z_scores.items()}\n",
    "\n",
    "time_centers = tim - stim_on_time\n",
    "depths_centers = depths + d_bin/2\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_right = z_scores['right']\n",
    "z_score_left = z_scores['left']\n",
    "######################\n",
    "# Get depth information\n",
    "ids = []\n",
    "acronyms = []\n",
    "true_depths = []\n",
    "ch_indexs = []\n",
    "cordinates = []\n",
    "for depth in depths_centers:\n",
    "    id = channels[channels['axial_um'] == depth]['atlas_id']\n",
    "    acronym = channels[channels['axial_um'] == depth]['acronym']\n",
    "    ch_index = channels[channels['axial_um'] == depth].index\n",
    "    coordinate = channels[channels['axial_um'] == depth][['x', 'y', 'z']]\n",
    "    if len(id) > 0:\n",
    "        id = id.values[0]\n",
    "        acro = acronym.values[0]\n",
    "        ch_index = ch_index[0]\n",
    "        coordinate = coordinate.values[0]\n",
    "        ch_indexs.append(ch_index)\n",
    "        acronyms.append(acro)\n",
    "        ids.append(id)\n",
    "        true_depths.append(depth)\n",
    "        cordinates.append(coordinate)\n",
    "ids = np.array(ids)\n",
    "acronyms = np.array(acronyms)\n",
    "ch_indexs = np.array(ch_indexs)\n",
    "coordinates = np.array(cordinates)\n",
    "trial_indx = np.array(trial_indx)\n",
    "distance_to_change = np.array(distance_to_change)\n",
    "probs_left = np.array(probs_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_coords shape: (37248,)\n",
      "y_coords shape: (37248,)\n"
     ]
    }
   ],
   "source": [
    "average_period = [0.1, 1]\n",
    "time_indices = np.where((time_centers >= average_period[0]) & (time_centers <= average_period[1]))[0]\n",
    "\n",
    "# Average firing rates over the defined time window\n",
    "z_score_right_average = np.mean(z_score_right[:, :, time_indices], axis=2)\n",
    "z_score_left_average = np.mean(z_score_left[:, :, time_indices], axis=2)\n",
    "\n",
    "# Labels for right (1) and left (0) trials\n",
    "labels_c1 = np.ones(z_score_right.shape[0])\n",
    "labels_c2 = np.zeros(z_score_left.shape[0])\n",
    "labels = np.concatenate((labels_c1, labels_c2))\n",
    "\n",
    "# Stack firing rates data along trial dimension\n",
    "firing_rates = np.concatenate((z_score_right_average, z_score_left_average), axis=0)\n",
    "\n",
    "# Expand metadata to match firing rates data shape\n",
    "x_coords = np.tile(coordinates[:, 0], firing_rates.shape[0])\n",
    "y_coords = np.tile(coordinates[:, 1], firing_rates.shape[0])\n",
    "z_coords = np.tile(coordinates[:, 2], firing_rates.shape[0])\n",
    "acronyms_repeated = np.tile(acronyms, firing_rates.shape[0])\n",
    "ch_indexs_repeated = np.tile(ch_indexs, firing_rates.shape[0])\n",
    "labels_repeated = np.tile(labels, firing_rates.shape[1])\n",
    "trial_indx_repeated = np.tile(trial_indx, firing_rates.shape[1])\n",
    "distance_to_change_repeated = np.tile(distance_to_change, firing_rates.shape[1])\n",
    "probs_left_repeated = np.tile(probs_left, firing_rates.shape[1])\n",
    "\n",
    "# Repeat probe and experiment IDs to match data structure\n",
    "probe_ids = np.repeat(pid, firing_rates.size)\n",
    "experiment_ids = np.repeat(eid, firing_rates.size)\n",
    "\n",
    "\n",
    "# print the shape of all the variables \n",
    "print(f'x_coords shape: {x_coords.shape}')\n",
    "print(f'y_coords shape: {y_coords.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original array shapes:\n",
      "firing_rates: (194, 192)\n",
      "labels_repeated: (37248,)\n",
      "trial_indx_repeated: (37248,)\n",
      "distance_to_change_repeated: (37248,)\n",
      "probs_left_repeated: (37248,)\n",
      "x_coords: (37248,)\n",
      "y_coords: (37248,)\n",
      "z_coords: (37248,)\n",
      "ch_indexs_repeated: (37248,)\n",
      "acronyms_repeated: 37248\n",
      "probe_ids: 37248\n",
      "experiment_ids: 37248\n"
     ]
    }
   ],
   "source": [
    "# Check shapes of original arrays before flattening/repeating\n",
    "print(\"Original array shapes:\")\n",
    "print(f\"firing_rates: {firing_rates.shape}\")\n",
    "print(f\"labels_repeated: {labels_repeated.shape}\")\n",
    "print(f\"trial_indx_repeated: {trial_indx_repeated.shape}\")\n",
    "print(f\"distance_to_change_repeated: {distance_to_change_repeated.shape}\")\n",
    "print(f\"probs_left_repeated: {probs_left_repeated.shape}\")\n",
    "print(f\"x_coords: {x_coords.shape}\")\n",
    "print(f\"y_coords: {y_coords.shape}\")\n",
    "print(f\"z_coords: {z_coords.shape}\")\n",
    "print(f\"ch_indexs_repeated: {ch_indexs_repeated.shape}\")\n",
    "print(f\"acronyms_repeated: {len(acronyms_repeated)}\")  # if it's a list\n",
    "print(f\"probe_ids: {len(probe_ids)}\")  # if it's a list\n",
    "print(f\"experiment_ids: {len(experiment_ids)}\")  # if it's a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores_new = {}\n",
    "for stim_type, z_score_list in z_scores.items():\n",
    "    n = np.array(z_score_list)\n",
    "    z_scores_new[stim_type] = z_score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = {stim_type: [] for stim_type in stim_events.keys()}\n",
    "\n",
    "for stim_type, stim_times in stim_events.items():\n",
    "    stim_times = stim_times[~np.isnan(stim_times)]\n",
    "    for stim_on_time in stim_times:\n",
    "        interval = [stim_on_time - pre_stim, stim_on_time + post_stim]\n",
    "        idx = np.where((spike_times > interval[0]) & (spike_times < interval[1]))[0]\n",
    "        spike_times_i = spike_times[idx]\n",
    "        spike_depths_i = spike_depths[idx]\n",
    "\n",
    "        # Create binned array\n",
    "        binned_array, t, depths = bin_spike_data(spike_times_i, spike_depths_i, t_bin, d_bin, time_range=interval, depth_range=depth_lim)\n",
    "\n",
    "        # Compute baseline mean and std\n",
    "        baseline_mean = np.mean(binned_array[:, :int(pre_stim/t_bin)], axis=1)\n",
    "        baseline_std = np.std(binned_array[:, :int(pre_stim/t_bin)], axis=1)\n",
    "        baseline_std[baseline_std == 0] = 1  # Avoid division by zero\n",
    "\n",
    "        # Compute Z-score of firing rates\n",
    "        z_score_firing_rate = (binned_array - baseline_mean[:, np.newaxis]) / baseline_std[:, np.newaxis]\n",
    "        print(z_score_firing_rate.shape)\n",
    "        # Append to the respective list\n",
    "        z_scores[stim_type].append(z_score_firing_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 8c732bf2-639d-496c-bf82-464bc9c2d54b ebce500b-c530-47de-8cb1-963c552703ea active\n",
      "Processing 8c732bf2-639d-496c-bf82-464bc9c2d54b ebce500b-c530-47de-8cb1-963c552703ea passive\n",
      "Processing 8c732bf2-639d-496c-bf82-464bc9c2d54b ebce500b-c530-47de-8cb1-963c552703ea done\n"
     ]
    }
   ],
   "source": [
    "from firing_rate import *\n",
    "# load the json file\n",
    "import json\n",
    "with open('whole_brain_classification/pid_eid_pairs.json', 'r') as f:\n",
    "    pid_eid_pairs = json.load(f)\n",
    "\n",
    "pid_eid_pair = pid_eid_pairs[0]\n",
    "\n",
    "data = merg_active_passive(pid_eid_pair, t_bin=0.3, pre_stim=0.3, post_stim=0.3,  modee = 'download', min_contrast=1, probability_left='all', min_time = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "firing_rates, trial_info, ch_info, time_info = data['firing_rates'], data['trial_info'], data['depth_info'], data['time_bins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    }
   ],
   "source": [
    "right = len(trial_info[trial_info['labels'] == 1 ])\n",
    "print(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# import pickle\n",
    "# import pandas as pd\n",
    "\n",
    "# base_path = '/mnt/data/AdaptiveControl/IBLrawdata/classification/preprocess_firingRate'\n",
    "# pid= pid_eid_pairs[0][0]\n",
    "# path = os.path.join(base_path, f'{pid}.pkl')\n",
    "# if not os.path.exists(path):\n",
    "#     print(f\"File {path} not found\")\n",
    "#     raise FileNotFoundError\n",
    "\n",
    "# with open(path, 'rb') as f:\n",
    "#     data = pickle.load(f)\n",
    "# firing_rates = data['firing_rates'] # shape (n_trials, n_neurons, n_time_bins)\n",
    "# trial_info = data['trial_info'] # dictionary keys ['trial_index', 'labels', 'contrasts', 'distance_to_change', 'prob_left', 'probe_id', 'experiment_id']\n",
    "# depth_info = data['depth_info'] # dictionary keys ['depth', 'ids', 'acronyms', 'ch_indexs', 'x_coordinates', 'y_coordinates', 'z_coordinates']\n",
    "# time_bins = data['time_bins'] # shape (n_time_bins,)\n",
    "\n",
    "# # # Filter conditions\n",
    "# # contrast_filter = trial_info['contrasts'] == 1\n",
    "# # probability_left_filter = (trial_info['prob_left'] == 0.5) | np.isnan(trial_info['prob_left'])\n",
    "# # trial_info_filter = contrast_filter & probability_left_filter\n",
    "# # time_filter = time_bins >= 0  # time is in milliseconds\n",
    "\n",
    "# # # Apply filters\n",
    "# # trial_info_filtered_dict = {key: value[trial_info_filter] for key, value in trial_info.items()}\n",
    "# # trial_info_filtered = pd.DataFrame(trial_info_filtered_dict)\n",
    "# # time_bins_filtered = time_bins[time_filter]\n",
    "# # firing_rates_filtered = firing_rates[trial_info_filter, :, :][:, :, time_filter]\n",
    "\n",
    "# # # labels_filter = trial_info_filtered['labels']\n",
    "# # # from compute_decodability import compute_decodability_LR\n",
    "# # # decodability_scores, decodability_std = compute_decodability_LR(firing_rates_filtered, labels_filter)\n",
    "# # firingRates = firing_rates_filtered\n",
    "# # labels = trial_info_filtered['labels']\n",
    "# # n_trials, n_depths, n_time = firingRates.shape\n",
    "# # k_folds = 5\n",
    "# # import numpy as np\n",
    "# # from imblearn.under_sampling import RandomUnderSampler\n",
    "# # from sklearn.linear_model import LogisticRegression\n",
    "# # from sklearn.metrics import roc_auc_score\n",
    "# # from sklearn.model_selection import StratifiedKFold\n",
    "# # from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# # # Initialize storage for decodability scores\n",
    "# # decodability_scores = np.zeros((n_depths, n_time))\n",
    "# # decodability_std = np.zeros((n_depths, n_time))\n",
    "\n",
    "# # for depth in range(n_depths):\n",
    "# #     for time in range(n_time):\n",
    "# #         # Extract firing rates for the current depth and time point\n",
    "# #         firing_rates = firingRates[:, depth, time]\n",
    "\n",
    "# #         # Reshape firing rates and labels to use with RandomUnderSampler\n",
    "# #         firing_rates_reshaped = firing_rates.reshape(-1, 1)\n",
    "# #         labels_reshaped = labels\n",
    "\n",
    "# #         # Handle class imbalance using RandomUnderSampler\n",
    "# #         rus = RandomUnderSampler(random_state=42)\n",
    "# #         firing_rates_balanced, labels_balanced = rus.fit_resample(firing_rates_reshaped, labels_reshaped)\n",
    "\n",
    "# #         # Standardize features\n",
    "# #         scaler = StandardScaler()\n",
    "# #         firing_rates_scaled = scaler.fit_transform(firing_rates_balanced)\n",
    "\n",
    "# #         # Initialize cross-validation\n",
    "# #         skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "# #         fold_auroc = []\n",
    "\n",
    "# #         for train_index, test_index in skf.split(firing_rates_scaled, labels_balanced):\n",
    "# #             X_train, X_test = firing_rates_scaled[train_index], firing_rates_scaled[test_index]\n",
    "# #             y_train, y_test = labels_balanced[train_index], labels_balanced[test_index]\n",
    "\n",
    "# #             # Fit Logistic Regression\n",
    "# #             model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# #             model.fit(X_train, y_train)\n",
    "\n",
    "# #             # Predict probabilities and calculate AUROC\n",
    "# #             predictions = model.predict(X_test)\n",
    "# #             auroc = roc_auc_score(y_test, predictions, average=None)\n",
    "# #             fold_auroc.append(auroc)\n",
    "\n",
    "# #         # Store the average AUROC and standard deviation across folds\n",
    "# #         decodability_scores[depth, time] = np.mean(fold_auroc)\n",
    "# #         decodability_std[depth, time] = np.std(fold_auroc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor job status\n",
    "total_jobs = len(jobs)\n",
    "print(f\"Submitted {total_jobs} jobs.\")\n",
    "\n",
    "finished_jobs = 0\n",
    "completed_jobs_set = set()\n",
    "\n",
    "while finished_jobs < total_jobs:\n",
    "    for idx, job in enumerate(jobs):\n",
    "        if job.done() and idx not in completed_jobs_set:\n",
    "            finished_jobs += 1\n",
    "            completed_jobs_set.add(idx)\n",
    "            print(f\"Jobs finished: {finished_jobs}/{total_jobs}\")\n",
    "\n",
    "print(\"All jobs are finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_file = []\n",
    "success = []\n",
    "unsuccess_jobs = []\n",
    "for job in jobs:\n",
    "    try:\n",
    "        result = job.result()\n",
    "        if result == 1:\n",
    "            no_file.append(job)\n",
    "        elif result == 0:\n",
    "            success.append(job)\n",
    "    except:\n",
    "        unsuccess_jobs.append(job)\n",
    "        continue\n",
    "print(f'no_file: {len(no_file)}')\n",
    "print(f'Success jobs: {len(success)}')\n",
    "print(f'Unsuccess jobs: {len(unsuccess_jobs)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (f'No path: {len(no_path)}')\n",
    "print (f'No decoded: {len(no_decoded)}')\n",
    "print (f'All acronyms: {len(acronyms_all)}')\n",
    "print (f'All decodability scores: {len(decodability_scores_mean_all)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "no_path = []\n",
    "no_decoded = []\n",
    "acronyms_all = []\n",
    "decodability_scores_mean_all = []\n",
    "decodability_scores_max_all = []\n",
    "for pid, eid in pid_eid_pairs:\n",
    "    base_path = '/mnt/data/AdaptiveControl/IBLrawdata/classification/preprocess_firingRate'\n",
    "    path = os.path.join(base_path, f'{pid}.pkl')\n",
    "    if not os.path.exists(path):\n",
    "        no_path.append(pid)\n",
    "        continue\n",
    "    import pickle\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    decoded = [ key for key in data.keys() if 'decodability_scores' in key]\n",
    "    if len(decoded) == 0:\n",
    "        no_decoded.append(pid)\n",
    "        continue\n",
    "    depth_info = data['depth_info']\n",
    "    acronyms = depth_info['acronyms']\n",
    "    decodability_scores = data['decodability_scores'][:, 1:-5]\n",
    "    # check if any value is over 0.6\n",
    "    # if np.any(decodability_scores > 0.6):\n",
    "    #     print(pid)\n",
    "    \n",
    "    decodability_scores_mean = decodability_scores.mean(axis=1)\n",
    "    decodability_scores_max = decodability_scores.max(axis=1)\n",
    "    decodability_scores_mean_list = decodability_scores_mean.tolist()\n",
    "    decodability_scores_max_list = decodability_scores_max.tolist()\n",
    "    acronyms_list = acronyms.tolist()\n",
    "    decodability_scores_mean_all.extend(decodability_scores_mean_list)\n",
    "    acronyms_all.extend(acronyms_list)\n",
    "    decodability_scores_max_all.extend(decodability_scores_max_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "visual_regions = ['VISp', 'VISpm', 'VISam', 'VISa', 'VISrl', 'VISal', 'VISli', 'VISl']\n",
    "filtered_indices = [i for i, region in enumerate(acronyms_all) if any(visual in region for visual in visual_regions)]\n",
    "\n",
    "acronyms_filtered = [acronyms_all[i] for i in filtered_indices]\n",
    "unique_acronyms_filtered = np.unique(acronyms_filtered)\n",
    "decodability_scores_mean_filtered = [decodability_scores_max_all[i] for i in filtered_indices]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the threshold for decodability\n",
    "decoding_threshold = 0.6\n",
    "\n",
    "# Initialize a dictionary to store counts of scores above the threshold for each acronym\n",
    "acronym_decoding_counts = {acronym: 0 for acronym in unique_acronyms_filtered}\n",
    "\n",
    "# Count the number of scores above the threshold for each unique acronym\n",
    "for acronym, score in zip(acronyms_filtered, decodability_scores_mean_filtered):\n",
    "    if score > decoding_threshold:\n",
    "        acronym_decoding_counts[acronym] += 1\n",
    "\n",
    "# Extract the acronyms and their corresponding counts for plotting\n",
    "acronyms = list(acronym_decoding_counts.keys())\n",
    "decoding_counts = list(acronym_decoding_counts.values())\n",
    "\n",
    "# Plot the number of decodings over the threshold for each unique acronym\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(acronyms, decoding_counts, color='skyblue')\n",
    "plt.xlabel('Acronyms')\n",
    "plt.ylabel('Number of Decodings > 0.7')\n",
    "plt.title('Number of Decodings Over 0.7 in Each Unique Acronym')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
