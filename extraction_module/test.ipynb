{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from config import  BRAIN_REGION_PAIRS, SUBMITIT_PARAMS, extraction_parameters\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import submitit\n",
    "import gc\n",
    "import config\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "from data_extraction import data_extraction\n",
    "\n",
    "\n",
    "data_extraction_with_params = partial(data_extraction, **extraction_parameters)\n",
    "##########################\n",
    "# Submit jobs\n",
    "##########################\n",
    "\n",
    "if len(BRAIN_REGION_PAIRS) >1 :\n",
    "    raise ValueError(\"BRAIN_REGION_PAIRS should have only one pair of brain regions for extraction\")\n",
    "\n",
    "region1 = BRAIN_REGION_PAIRS[0][0]\n",
    "region2 = BRAIN_REGION_PAIRS[0][1]\n",
    "\n",
    "table_path = os.path.join(os.getcwd(), f'data/eid_probe_info_{region1}_{region2}.csv')\n",
    "if not os.path.isfile(table_path):\n",
    "    print(f\"eid_probe_info_{region1}_{region2}.csv does not exist at {os.getcwd()}\")\n",
    "    print(\"use eid_probe_info_table.py to generate the table\")\n",
    "    raise ValueError(f\"table does not exist at {os.getcwd()}\")\n",
    "\n",
    "# Load the table\n",
    "df = pd.read_csv(table_path)\n",
    "job_args = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    # if i>0:\n",
    "    #     break\n",
    "    eid = row['eid']\n",
    "    probe_label_1 = row[f'label_{region1}'][2:-2]\n",
    "    probe_label_2 = row[f'label_{region2}'][2:-2]\n",
    "    probe_labels = [probe_label_1, probe_label_2]\n",
    "    probe_labels = list(set(probe_labels))\n",
    "    \n",
    "\n",
    "    for probe_label in probe_labels:\n",
    "        job_args.append((eid, probe_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "eids = os.listdir('/mnt/data/AdaptiveControl/IBLrawdata/eid_data/') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = one.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = one.load_dataset(eid, 'alf/_ibl_trials.table.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dset in dsets:\n",
    "    if 'trials' in dset:\n",
    "        print(dset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for eid in eids:\n",
    "    behavior = one.load_dataset(eid, 'trials')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.api import ONE\n",
    "ONE.setup(base_url='https://openalyx.internationalbrainlab.org', silent=True)\n",
    "one = ONE(password='international')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove log directory\n",
    "import shutil\n",
    "log_folder = os.path.join(os.getcwd(), 'logs')\n",
    "if os.path.isdir(log_folder):\n",
    "    shutil.rmtree(log_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>file_size</th>\n",
       "      <th>hash</th>\n",
       "      <th>default_revision</th>\n",
       "      <th>qc</th>\n",
       "      <th>exists_flatiron</th>\n",
       "      <th>exists_aws</th>\n",
       "      <th>exists</th>\n",
       "      <th>session_path</th>\n",
       "      <th>rel_path</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eid</th>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file_size, hash, default_revision, qc, exists_flatiron, exists_aws, exists, session_path, rel_path]\n",
       "Index: []"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid = '8185f1e9-cfe0-4fd6-8d7e-446a8051c588'\n",
    "one.list_datasets(pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intbrainlab\n"
     ]
    }
   ],
   "source": [
    "print(one.alyx.user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one.list_datasets(eid, collection='alf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running job for e26c6001-defe-42a9-9ded-368e3f03ac61 and probe00\n",
      "eid: e26c6001-defe-42a9-9ded-368e3f03ac61\n",
      "probe_label: probe00\n",
      "kwargs: {'extract_wheel': True, 'extract_dlc': True, 'extract_spikes': True, 'extract_lfp': True, 'overwrite_behavior': False, 'overwrite_wheel': False, 'overwrite_dlc': False, 'overwrite_spikes': False, 'overwrite_lfp': False, 'resampled_fs': 500}\n",
      "Extracting data for eid e26c6001-defe-42a9-9ded-368e3f03ac61 and probe 8185f1e9-cfe0-4fd6-8d7e-446a8051c588...\n",
      "Extracting behavior data...\n"
     ]
    },
    {
     "ename": "ALFObjectNotFound",
     "evalue": "Dataset \"alf/_ibl_trials.table.pqt\" not found \n The ALF object was not found.  This may occur if the object or namespace or incorrectly formatted e.g. the object \"_ibl_trials.intervals.npy\" would be found with the filters `object=\"trials\", namespace=\"ibl\"` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mALFObjectNotFound\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m eid, probe_label \u001b[38;5;241m=\u001b[39m job_args[\u001b[38;5;241m25\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning job for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprobe_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdata_extraction_with_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43meid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprobe_label\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/crnldata/cophy/TeamProjects/mohammad/ibl-oscillations/_analyses/extraction_module/data_extraction.py:69\u001b[0m, in \u001b[0;36mdata_extraction\u001b[0;34m(eid, probe_label, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtracting behavior data...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path_be) \u001b[38;5;129;01mor\u001b[39;00m overwrite_behavior:\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# behavior = one.load_object(eid, 'trials', collection='alf')\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     behavior \u001b[38;5;241m=\u001b[39m \u001b[43mone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43meid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malf/_ibl_trials.table.pqt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     behavior_df \u001b[38;5;241m=\u001b[39m behavior\u001b[38;5;241m.\u001b[39mto_df()\n\u001b[1;32m     71\u001b[0m     behavior_df\u001b[38;5;241m.\u001b[39mto_pickle(file_path_be)\n",
      "File \u001b[0;32m~/miniconda3/envs/iblenv/lib/python3.10/site-packages/one/util.py:163\u001b[0m, in \u001b[0;36mrefresh.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh_cache(mode\u001b[38;5;241m=\u001b[39mmode)\n\u001b[0;32m--> 163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iblenv/lib/python3.10/site-packages/one/util.py:149\u001b[0m, in \u001b[0;36mparse_id.<locals>.wrapper\u001b[0;34m(self, id, *args, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eid \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot parse session ID \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mid\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (session may not exist)\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/iblenv/lib/python3.10/site-packages/one/api.py:1109\u001b[0m, in \u001b[0;36mOne.load_dataset\u001b[0;34m(self, eid, dataset, collection, revision, query_type, download_only, check_hash)\u001b[0m\n\u001b[1;32m   1106\u001b[0m datasets \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mfilter_datasets(datasets, dataset, collection, revision,\n\u001b[1;32m   1107\u001b[0m                                 wildcards\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwildcards, assert_unique\u001b[38;5;241m=\u001b[39massert_unique)\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(datasets) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m alferr\u001b[38;5;241m.\u001b[39mALFObjectNotFound(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Check files exist / download remote files\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m offline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mALFObjectNotFound\u001b[0m: Dataset \"alf/_ibl_trials.table.pqt\" not found \n The ALF object was not found.  This may occur if the object or namespace or incorrectly formatted e.g. the object \"_ibl_trials.intervals.npy\" would be found with the filters `object=\"trials\", namespace=\"ibl\"` "
     ]
    }
   ],
   "source": [
    "eid, probe_label = job_args[25]\n",
    "print(f\"Running job for {eid} and {probe_label}\")\n",
    "\n",
    "data_extraction_with_params(eid, probe_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.api import ONE\n",
    "ONE.setup(base_url='https://openalyx.internationalbrainlab.org', silent=True)\n",
    "one = ONE(password='international')\n",
    "from detect_bad_channels import detect_bad_channels\n",
    "import os \n",
    "from config import  BRAIN_REGION_PAIRS, SUBMITIT_PARAMS, extraction_parameters\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "from brainbox.io.one import SpikeSortingLoader\n",
    "from iblatlas.atlas import AllenAtlas\n",
    "from iblatlas.regions import BrainRegions\n",
    "import ibldsp.voltage as voltage\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.api import ONE\n",
    "ONE.setup(base_url='https://openalyx.internationalbrainlab.org', silent=True)\n",
    "one = ONE(password='international')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from config import  BRAIN_REGION_PAIRS, SUBMITIT_PARAMS, extraction_parameters\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mne\n",
    "import submitit\n",
    "import gc\n",
    "import config\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "from data_extraction import data_extraction\n",
    "\n",
    "\n",
    "data_extraction_with_params = partial(data_extraction, **extraction_parameters)\n",
    "##########################\n",
    "# Submit jobs\n",
    "##########################\n",
    "\n",
    "if len(BRAIN_REGION_PAIRS) >1 :\n",
    "    raise ValueError(\"BRAIN_REGION_PAIRS should have only one pair of brain regions for extraction\")\n",
    "\n",
    "region1 = BRAIN_REGION_PAIRS[0][0]\n",
    "region2 = BRAIN_REGION_PAIRS[0][1]\n",
    "\n",
    "table_path = os.path.join(os.getcwd(), f'eid_probe_info_{region1}_{region2}.csv')\n",
    "if not os.path.isfile(table_path):\n",
    "    print(f\"eid_probe_info_{region1}_{region2}.csv does not exist at {os.getcwd()}\")\n",
    "    print(\"use eid_probe_info_table.py to generate the table\")\n",
    "    raise ValueError(f\"table does not exist at {os.getcwd()}\")\n",
    "\n",
    "# Load the table\n",
    "df = pd.read_csv(table_path)\n",
    "job_args = []\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    if i>0:\n",
    "        break\n",
    "    eid = row['eid']\n",
    "    probe_label_1 = row[f'label_{region1}'][2:-2]\n",
    "    probe_label_2 = row[f'label_{region2}'][2:-2]\n",
    "    probe_labels = [probe_label_1, probe_label_2]\n",
    "    probe_labels = list(set(probe_labels))\n",
    "    \n",
    "\n",
    "    for probe_label in probe_labels:\n",
    "        job_args.append((eid, probe_label))\n",
    "\n",
    "\n",
    "print(f'Number of jobs to submit: {len(job_args)}')\n",
    "log_dir  = config.log_dir\n",
    "max_jobs_parallel = config.maxjobs\n",
    "print(f'Maximum number of jobs to run in parallel: {max_jobs_parallel}')\n",
    "executor = submitit.AutoExecutor(folder=log_dir)\n",
    "executor.update_parameters(**config.SUBMITIT_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from one.api import ONE\n",
    "ONE.setup(base_url='https://openalyx.internationalbrainlab.org', silent=True)\n",
    "one = ONE(password='international')\n",
    "eid = '5c0c560e-9e1f-45e9-b66e-e4ee7855be84'\n",
    "pids, labels = one.eid2pid(eid)\n",
    "probe_label = 'probe01'\n",
    "\n",
    "pid = next((pid for pid, label in zip(pids, labels) if label == probe_label), None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(zip(pids[0], pids[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iblenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
