<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.489">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>all_included_version</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="all_included_version_files/libs/clipboard/clipboard.min.js"></script>
<script src="all_included_version_files/libs/quarto-html/quarto.js"></script>
<script src="all_included_version_files/libs/quarto-html/popper.min.js"></script>
<script src="all_included_version_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="all_included_version_files/libs/quarto-html/anchor.min.js"></script>
<link href="all_included_version_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="all_included_version_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="all_included_version_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="all_included_version_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="all_included_version_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">




<section id="introduction" class="level1">
<h1>Introduction</h1>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>How does the brain efficiently process the overwhelming amount of sensory input that it receives from a constantly changing and uncertain environment? According to the predictive coding (PC) framework, the brain’s key solution is to actively predict incoming sensory input based on past observations and to prioritize the processing of unpredicted sensory information <span class="citation" data-cites="huang2011">(<a href="#ref-huang2011" role="doc-biblioref">Huang and Rao 2011</a>)</span>. PC envisions the brain as a prediction machine, where predictions are made through prior experience or expectation. These predictions are compared to the actual sensory input, and the difference, known as prediction error, is used to refine the internal model of the environment and orient attention towards unexpected features of the input. This allows the brain to minimize the prediction error and allocate its finite resources more efficiently <span class="citation" data-cites="vinck2022">(<a href="#ref-vinck2022" role="doc-biblioref">Vinck, Uran, and Canales-Johnson 2022</a>)</span>.</p>
<p>Predictive coding is generally thought to be implemented by the cortex. Indeed, the mammalian cortex is organized hierarchically, with lower level areas processing more basic sensory features and higher areas integrating this information into more complex representations <span class="citation" data-cites="felleman1991">(<a href="#ref-felleman1991" role="doc-biblioref">Felleman and Van Essen 1991</a>)</span>. Visual cortex is a prime example of this hierarchy, with primary visual cortex (V1) processing simple visual features like contrast and edges, and higher visual areas(e.g.&nbsp;V2, V4) extracting more complex features of stimuli like texture and shape [XSX]. The hierarchy of cortical processing allows the brain to make predictions at multiple levels of abstraction and compare these predictions to the actual sensory input. according to PC the communication between hierarchical cortical areas occurs through feedforward (FF) and feedback (FB) connections. FB projections involve top-down propagation of prediction from higher to lower area while FF projections involve bottom up assembly of sensory input and prediction error from lower to higher areas to update the internal model. FF and FB connections have distinct laminar origins and targets within the cortex. FF connections mainly originate from layers 3 and 5 (of lower areas) and target the layer 4 (of higher area). while, FB connections arise from layers 2 and 6 (of higher area) and target all layers except for layer 4 (of lower area)<span class="citation" data-cites="vinck2022">(<a href="#ref-vinck2022" role="doc-biblioref">Vinck, Uran, and Canales-Johnson 2022</a>)</span>.</p>
<p>Prominent studies in primates visual systems <span class="citation" data-cites="vankerkoerle2014">(<a href="#ref-vankerkoerle2014" role="doc-biblioref">Van Kerkoerle et al. 2014</a>)</span> suggest neural oscillations might play an important role in communication between cortical regions through FF-FB connections. The studies found that FF propagation is associated with gamma-band oscillations, while FB propagation involves alpha-band (8–12 Hz) oscillations. Similarly, a study on mice <span class="citation" data-cites="aggarwal2022">(<a href="#ref-aggarwal2022" role="doc-biblioref">Aggarwal et al. 2022</a>)</span> vision revealed FF waves in the 30–50 Hz range and FB waves in the 3-6 Hz range, where the phase of the FB oscillations modulated the amplitude of the FF oscillations. However, this was the only study on mice, to the best of my knowledge, that has investigated the FF-FB waves in the visual system. Additionally, the study utilized a simple visual stimulus that did not involve any prediction and higher-level cognitive processes.</p>
<p>The mouse visual system offers several advantages for studying predictive coding. The simpler and well-studied visual processing hierarchy in mice allows for more straightforward analysis of neural data. Genetic manipulation tools in mice also provide unique opportunities to explore cortical micro-circuitry and manipulate neural pathways, which is challenging in primates. Additionally, mouse studies are more cost-effective and logistically feasible compare to primates, making them an attractive model for large-scale explorations of complex brain functions like predictive coding.</p>
<p>The International Brain Laboratory (IBL) <span class="citation" data-cites="benson2023">(<a href="#ref-benson2023" role="doc-biblioref">Benson et al. 2023</a>)</span> provides an extensive open-access dataset recorded from more than 100 mice trained to perform a perceptual decision-making task. In this task, mice are presented with a visual stimulus of controlled contrast and are required to move the stimulus to the center of the screen using a steering wheel. The stimulus appears on the right or left side of the screen, with a fixed probability for blocks of trials to create a predictable yet changing probability that the correct response involves a rightward or leftward movement of the wheel. This latter feature is very interesting to study the neural implementation of predictive coding given since leveraging the block-dependent bias in trial distribution requires to constantly update a prior estimating how likely right or left stimuli are before they are even displayed.</p>
</section>
<section id="current-project" class="level2">
<h2 class="anchored" data-anchor-id="current-project">Current project</h2>
<p>So far, IBL researchers have focused most of their efforts on the detection, sorting and analysis of action potentials and firing rates, without prioritizing a specific brain region or a specific cognitive process. The overarching goal of the current project is to go beyond their data-driven analyses and to use the IBL dataset as a benchmark to study the neural implementation of predictive coding in the mammalian brain. In this perspective, we were particularly interested in local field potentials (LFP) and oscillatory electrophysiological signals in the theta (typically, between 2 to 7 Hz), alpha (7 to 15 Hz), beta (15 to 40 Hz) and gamma (40 to 200Hz).</p>
<p>Until recently, LFPs recorded in the IBL context have been left almost unexplored and the analysis of LFP signals produced by Neuropixel probes (as used by all IBL experiments) has received limited attention <span class="citation" data-cites="windolf2023">(<a href="#ref-windolf2023" role="doc-biblioref">Windolf et al. 2023</a>)</span>. In other words, the current project represents a first step towards unlocking the great potential of the IBL dataset to constrain theories of predictive coding and brain communication through oscillations.</p>
<p>Thus, my first goal was to assess the quality and the usability of IBL LFP data through the careful development of preprocessing pipelines and the comparison of different artifact detection and signal re-referencing methods. My second goal was to confirm that a core principle of the communication through oscillations framework could be observed in the dataset. More precisely, we hypothesized that the visual cortex of mice would display a phase-amplitude coupling analogous to that observed in primates, with the phase of slow oscillation (theta or alpha) modulating dynamically the amplitude of a fast oscillation (gamma power) <span class="citation" data-cites="bonnefond2015">(<a href="#ref-bonnefond2015" role="doc-biblioref">Bonnefond and Jensen 2015</a>)</span>. My third goal was to replicate and extend previous findings suggesting that, in the mouse visual cortex, the functional properties of theta oscillation between 2.5 and 5 Hz match some of the properties of alpha oscillations (i.e., between 7 to 15 Hz) in primates, including an anti-correlation with local firing rates and decreased power upon visual stimulation <span class="citation" data-cites="nestvogel2022">Senzai, Fernandez-Ruiz, and Buzsáki (<a href="#ref-senzai2019" role="doc-biblioref">2019</a>)</span>.</p>
<!--# If your master requires you to state what are your personal contribution to the project, you can add the corresponding paragraph here, saying that you have implemented all the steps described below from database access to plotting, with my help for some aspects of the code and under the supervision of both Mathilde and I. -->
</section>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<section id="data-source-and-recording-details" class="level2">
<h2 class="anchored" data-anchor-id="data-source-and-recording-details">Data source and recording details</h2>
<section id="international-brain-laboratory-ibl" class="level3">
<h3 class="anchored" data-anchor-id="international-brain-laboratory-ibl">International Brain Laboratory (IBL)</h3>
<p>We used the open-access dataset from the International Brain Laboratory (IBL). IBL provides a comprehensive set of recordings collected from more than 100 mice across 11 laboratories performing a standardized perceptual decision-making task. Data are collected from 267 brain regions by inserting 547 Neuropixel probes covering most of the left hemisphere and spanning the forebrain, midbrain and cerebellum, as well as the right hindbrain <span class="citation" data-cites="benson2023">(<a href="#ref-benson2023" role="doc-biblioref">Benson et al. 2023</a>)</span>.</p>
</section>
<section id="task-detail" class="level3">
<h3 class="anchored" data-anchor-id="task-detail">Task detail</h3>
<p>In the IBL task (Figure 1), head-fixed mice had to move a visual stimulus to the center by turning a wheel with their front paws. At the start of each trial, the mouse was required to refrain from moving the wheel for a quiescence period lasting between 400 and 700 milliseconds. After this period, a visual stimulus (Gabor patch) appeared on either the left or right side of the screen accompanied by a 100-millisecond tone (5 kHz sine wave). If the mouse correctly moved the stimulus to the center by turning the wheel over 35°, it received a 3 µL water reward. Incorrect responses or failing to respond within 60 seconds resulted in a 500-millisecond burst of white noise and a timeout <span class="citation" data-cites="benson2023">(<a href="#ref-benson2023" role="doc-biblioref">Benson et al. 2023</a>)</span>. As shown in Figure 1c, mice typically responded quickly within 2 seconds. The stimulus is always presented for the first 1 second regardless of response time (RT). RT is defined as the time after stimulus when the wheel rotation exceeds the threshold.</p>
<p>The experiment began with 90 unbiased trials where the stimulus appeared equally on both sides. The stimulus contrast levels were presented in a ratio of [2:2:2:2:1] for contrasts [100%, 25%, 12.5%, 6%, 0%]. After this initial block, trials were organized into biased blocks where the likelihood of the stimulus appearing on one side was fixed at 20% for the left and 80% for the right in “right blocks” or vice versa in “left blocks.” These blocks consisted of 20 to 100 trials determined by a truncated geometric distribution with stimulus contrast levels ratio identical to those in the unbiased block. In 0% contrast trials where no stimulus was visible, the side assignment followed the block bias (e.g., right side for right blocks) <span class="citation" data-cites="benson2023">(<a href="#ref-benson2023" role="doc-biblioref">Benson et al. 2023</a>)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/task.png" class="img-fluid figure-img" width="621"></p>
<figcaption>a) Example session block diagram and IBL task. Each block of consecutive trials after 90 trials varied the probability of the stimulus being on the right side. B) A timeline of the main events and variables of the IBL task. After a quiescence period, stimulus appears on screen alongside a go cue tone. Mice had to bring the stimulus to the center by turning the wheel. When the wheel rotation reaches the threshold 35 ° or after 60 s of no response, positive or negative feedback is provided depending on the mice choice. (a) and (b) are extracted from <!--# DO NOT FORGET TO CITE THE REF HERE --> . C) Distribution of response time (RT) with color blue and stimulus offset time with color yellow relative to stimulus onset. Note that there is always a stimulus presented for the first 1 second even though the mice typically answer sooner.&nbsp;</figcaption>
</figure>
</div>
</section>
<section id="electrophysiological-recording" class="level3">
<h3 class="anchored" data-anchor-id="electrophysiological-recording">Electrophysiological recording</h3>
<p>The neural recordings were conducted using Neuropixel probes with 384 recording channels and 960 low-impedance sites on a single shank <span class="citation" data-cites="benson2023">(<a href="#ref-benson2023" role="doc-biblioref">Benson et al. 2023</a>)</span>. Neuropixel probes are advanced silicon-based neural recording devices designed for high-density recording of neural activity across large populations of neurons with precise spatial and temporal resolution <span class="citation" data-cites="jun2017">(<a href="#ref-jun2017" role="doc-biblioref">Jun et al. 2017</a>)</span>. After the recordings, electrode tracks were reconstructed by performing serial-section 2-photon microscopy. A region was then assigned to each recording site (and inferred single neurons) within the Allen Common Coordinate Framework <span class="citation" data-cites="benson2023">(<a href="#ref-benson2023" role="doc-biblioref">Benson et al. 2023</a>)</span>.</p>
</section>
</section>
<section id="preprocessing-of-electrophysiological-data" class="level2">
<h2 class="anchored" data-anchor-id="preprocessing-of-electrophysiological-data">Preprocessing of electrophysiological data</h2>
<section id="exclusion-of-channels-and-trials" class="level3">
<h3 class="anchored" data-anchor-id="exclusion-of-channels-and-trials">Exclusion of channels and trials</h3>
<p>Local field potential (LFP) datasets alongside their corresponding behavioral data and channel locations were extracted for sessions that included at least one channel in the primary visual cortex. The destriping function of the IBL Python toolbox was applied as the first step of preprocessing to correct for the biases induced by the sequential acquisition of the raw voltage traces <span class="citation" data-cites="unified2024">(<a href="#ref-unified2024" role="doc-biblioref">IBL 2024</a>)</span>. This was followed by downsampling from 1000 Hz to 500 Hz to decrease the size of the data files. Next, channels were excluded based on three criteria: (i) those not located the primary visual cortex, (ii) those displaying excessively high variance according to power spectral density, (iii) and those with an excessively low coherence with neighboring channels.</p>
<p>We faced an unexpected problem with IBL LFP data due to amplifier saturation. Indeed, Neuropixel probes (especially earlier version) have a limited dynamic range that was frequently exceeded during the task (in particular, when the animal licked the spout to harvest water reward). For the analysis of spikes, this issue is less problematic as it only prevent from detecting spikes during the saturation. However, for the analysis of LFP, it introduces very salient artifacts and dramatically increase inter-trial variance in power, amplitude and phase estimates, potentially leading to erroneous conclusions. Therefore, we designed a custom exclusion procedure tailored to capture this specific problem. Trials were excluded based on the skewness of the absolute value of their first-order temporal derivative (threshold set to 1.5). Indeed, high skewness values typically reflect the presence of sudden, large amplitude changes in an otherwise mostly flat signal. By excluding these trials, our analyses focus on more consistent and representative portions of the data, improving the reliability of the results. Unless specified otherwise, all remaining trials were included in the presented analyses (i.e., missed, incorrect and slow responses).</p>
<!--# Here, I say that all trials that had an ok skewness were kept in the analysis, unless specified otherwise. If you never specify otherwise, remove the "Unless specified otherwise" part.-->
</section>
<section id="common-average-reference" class="level3">
<h3 class="anchored" data-anchor-id="common-average-reference">Common average reference</h3>
<p>Unless otherwise specified, our electrophysiological analyses used a common average reference scheme. The common reference was recomputed as the mean of all channels of interest per animal (i.e., those located in the primary visual cortex), after excluding noisy channels). This approach was chosen to limit the influence of electrical potentials outside of visual areas as well as the influence of non-physiological noise.</p>
</section>
<section id="current-source-density-csd" class="level3">
<h3 class="anchored" data-anchor-id="current-source-density-csd">Current Source Density (CSD)</h3>
<p>To remove the effects of volume conduction on the LFP data and improve spatial resolution, we used Current Source Density (CSD) analysis. CSD is a technique that estimates the local current flow in the brain by calculating the second spatial derivative of the recorded potentials to reduce the influence of distant sources. First, the Euclidean distances between adjacent channels were computed using the channels’ location relative to the end of the probe (axial) and their location relative to the middle of the probe (lateral). The Euclidean distance between adjacent channels <span class="math inline">\(i\)</span> and <span class="math inline">\(i \pm 1\)</span> was calculated as:</p>
<p><span class="math display">\[
d_{i,i \pm 1} = \sqrt{(x_{i \pm 1} - x_i)^2 + (y_{i \pm 1} - y_i)^2}
\]</span></p>
<p>Where <span class="math inline">\(d_{i,i \pm 1}\)</span> is the distance between channel <span class="math inline">\(i\)</span> and its adjacent channel <span class="math inline">\(i+1\)</span> (next channel) or <span class="math inline">\(i-1\)</span> (previous channel), <span class="math inline">\(x_i\)</span> and <span class="math inline">\(x_{i \pm 1}\)</span> are the axial coordinates of the channels, <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_{i \pm 1}\)</span> are the lateral coordinates.</p>
<p>Then the second spatial derivative of the LFP signals was computed as:</p>
<p><span class="math display">\[
CSD_i = \frac{V_{i+1} - V_i}{d_{i, i+1}^2} - \frac{V_i - V_{i-1}}{d_{i, i}^2}
\]</span></p>
<p>Here <span class="math inline">\(CSD_i\)</span> represents the current source density at channel <span class="math inline">\(i\)</span>, <span class="math inline">\(V_i\)</span> is the voltage at channel <span class="math inline">\(i\)</span>, <span class="math inline">\(V_{i+1}\)</span> and <span class="math inline">\(V_{i-1}\)</span> are the voltages at the adjacent channels <span class="math inline">\(i+1\)</span> and <span class="math inline">\(i-1\)</span> respectively.</p>
<p>In one-dimensional CSD analysis, it is typically assumed that channels are spaced uniformly (i.e., <span class="math inline">\(d_{i i+1} = d_{i i-1}\)</span>). However, in this project, we accounted for non-uniform spacing to enhance accuracy and enable the removal of noisy channels without risking the spread of artifacts to adjacent channels. The python script for CSD computation can be found in the “CSD_computation” Jupyter notebook in the GitHub repository.</p>
</section>
</section>
<section id="time-frequency-power-analysis" class="level2">
<h2 class="anchored" data-anchor-id="time-frequency-power-analysis">Time-Frequency power analysis</h2>
<!--# This part is good, but I would separate TF and ITC in different subsections. It does not matter to have subsections of different lengths. In the TF section, you can also describe how the signal is transformed for analysis (%, Db, logDb, etc.) -->
<p>For the time-frequency analysis, we chose the multitaper method. This method is known to be well-suited for situations where specific frequency bands are not preselected and the goal is a broad exploration of all frequencies. Multitaper parameters were selected in a way where frequency resolution was prioritized slightly over temporal resolution, especially at lower frequencies. In this regard, power and phase were calculated using MNE’s multitaper function with the following parameters: a frequency range of 2-45 Hz with a step size of 0.5 Hz for the 2-10 Hz range and 1 Hz for frequencies above 10 Hz and a time-bandwidth product of 3.5 with the number of cycles at each frequency point set to half of the corresponding frequency (<span class="math inline">\(\text{n-cycles} = \frac{f}{2}\)</span>). These parameters were found to be optimal for our specific data and goals. A detailed comparison of different parameter settings can be found in the “<em>tf_resolution</em>” Jupyter notebook in the GitHub repository.</p>
<p>Baseline correction were applied to the time frequency data with baseline defined as the interval from -0.7 to -0.5 seconds relative to stimulus onset. For baseline correction, the percentage change method were used which can be expressed with the following formula:</p>
<p><span class="math display">\[
\text{Corrected Power}(t,f) = (\frac{P(t,f)-\text{Baseline}(f)}{\text{Baseline}(f)}) \times 100
\]</span></p>
<p>Where <span class="math inline">\(P(t,f)\)</span> is the power at a specific time <span class="math inline">\((t)\)</span> and frequency <span class="math inline">\((f)\)</span>, and <span class="math inline">\(\text{Baseline}(f)\)</span> is the averaged power within the baseline interval for each frequency.</p>
</section>
<section id="inter-trial-phase-coherence-itc" class="level2">
<h2 class="anchored" data-anchor-id="inter-trial-phase-coherence-itc">Inter trial Phase coherence (ITC)</h2>
<p>Inter-trial phase coherence (ITC) was computed using MNE’s built-in function. ITC is a measure of the consistency of the phase of a signal across different trials at a given time and frequency. Mathematically, ITC is calculated as the magnitude of the average of normalized complex phase values across trials. For each trial, the phase of the signal, denoted as <span class="math inline">\(\phi(f, t)\)</span>, is extracted at each frequency <span class="math inline">\(f\)</span> and time point <span class="math inline">\(t\)</span>. These phase values are then represented as unit vectors on the complex plane, i.e., <span class="math inline">\(e^{i\phi(f, t)}\)</span>.</p>
<p>The ITC at a particular time-frequency point is then defined as:</p>
<p><span class="math display">\[
\text{ITC}(f, t) = \left| \frac{1}{N} \sum_{n=1}^{N} e^{i\phi_n(f, t)} \right|
\]</span></p>
<p>where <span class="math inline">\(N\)</span> is the number of trials, and <span class="math inline">\(\phi_n(f, t)\)</span> is the phase at frequency <span class="math inline">\(f\)</span> and time <span class="math inline">\(t\)</span> for the <span class="math inline">\(n\)</span>-th trial. The resulting ITC value ranges from 0 to 1, where 0 indicates no phase consistency across trials, and 1 indicates perfect phase alignment across all trials.</p>
</section>
<section id="phase-amplitude-coupling" class="level2">
<h2 class="anchored" data-anchor-id="phase-amplitude-coupling">Phase-Amplitude Coupling</h2>
<p>Phase-Amplitude Coupling (PAC) quantifies the interaction between the phase and amplitude of two distinct frequency bands, typically involving the phase of a low-frequency oscillation and the amplitude of a high-frequency oscillation. In this study, PAC was computed for phase frequencies ranging from 2 to 7 Hz and amplitude frequencies from 25 to 80 Hz using the TensorPAC Python module <span class="citation" data-cites="combrisson2020">(<a href="#ref-combrisson2020" role="doc-biblioref">Combrisson et al. 2020</a>)</span>. The process begins with the extraction of the instantaneous phase of the low-frequency signal and the amplitude envelope of the high-frequency signal carried out through Morlet wavelets. The interaction between these signals is then evaluated to determine how the phase of slower oscillations modulates the amplitude of faster oscillations. In this project, the Gaussian Copula (GC) method was employed to compute PAC for a time window spanning 500 ms before stimulus onset to 1 second after the stimulus. Compared to other methods such as Phase Locking Value, GC is more robust to shifts in overall signal amplitude <span class="citation" data-cites="combrisson2020">(<a href="#ref-combrisson2020" role="doc-biblioref">Combrisson et al. 2020</a>)</span>.</p>
<p>The core of the GC method involves calculating the mutual information between normalized amplitude and phase to quantify the degree to which the phase of the low-frequency oscillation governs the amplitude of the high-frequency oscillation. This mutual information provides a lower-bound estimate of the PAC that is robust to overall amplitude shifts. Mathematically, this can be expressed as: <span class="math display">\[
gcPAC = I(a(t); \sin(\phi(t)), \cos(\phi(t)))
\]</span></p>
<p>Where <span class="math inline">\(I\)</span> denotes the mutual information, <span class="math inline">\(a(t)\)</span> represents the normalized amplitude signal, and <span class="math inline">\(\phi(t)\)</span> represents the normalized phase signal.</p>
<p>After computing PAC, the values were normalized for each channel using z-score normalization, which involves subtracting the mean and dividing by the standard deviation. This process standardizes the PAC values and makes them comparable across channels and subjects. Following normalization, the values were averaged across all frequencies and for two distinct time windows: before the stimulus (-0.5 to 0 seconds) and after the stimulus (0 to 1 second).</p>
</section>
<section id="statistical-analysis" class="level2">
<h2 class="anchored" data-anchor-id="statistical-analysis">Statistical Analysis</h2>
<section id="analysis-of-variance-anovas" class="level3">
<h3 class="anchored" data-anchor-id="analysis-of-variance-anovas">Analysis of variance (ANOVAs)</h3>
<!--#  A short paragraph to say where you did ANOVAs (indicate when and why you used non parametric Friedman).  -->
</section>
<section id="cluster-based-statistics" class="level3">
<h3 class="anchored" data-anchor-id="cluster-based-statistics">Cluster-based statistics</h3>
<!--# A short paragraph to say when and why (the why is mostly to deal with multiple comparison when multiple tests are highly correlated in space, time and or frequency). Add another short paragraph to say what is the process (you can probably get inspiration in the doc of MNE or fieldtrip  -->
</section>
<section id="multiple-comparison-for-time-frequency-itc-estimate" class="level3">
<h3 class="anchored" data-anchor-id="multiple-comparison-for-time-frequency-itc-estimate">multiple comparison for time frequency ITC estimate</h3>
<!--# MISSING FROM THE METHODS: clear criteria to exclude channels and trials: could be a subsection of preprocessing for example  -->
</section>
</section>
</section>
<section id="results" class="level1">
<h1>Results</h1>
<section id="data-summary" class="level2">
<h2 class="anchored" data-anchor-id="data-summary">Data summary</h2>
<p>A total of 63 probes were identified in the IBL datasets, with at least one channel assigned to the primary visual cortex (V1) (see fix X a;b for one insertion example). From the initial dataset, 7 and 15 insertions were excluded due to over 40% noisy channels and trials, respectively. In the end, 41 insertions were retained, consisting of 2,262 total channels and 25,075 trials. On average, each probe was associated with 54.83 channels in V1 (range: 2 to 118), with an average of 532.66 trials per session (range: 276 to 1,098) (see fig X d <!--# beware to double-check all these references to figures before submitting your thesis. Also, be consistent between calls: If you write Figure 1a somewhere, don't write Fig. 1A elsewhere! A better way to find quickly this information that should be filled later is to use "XXX" instead of just "X" -->) . Among the total number of channels, 212 (9.37%) were in layer 1, 456 (20.16%) in layer 2/3, 338 (14.94%) in layer 4, 650 (28.74%) in layer 5, and 606 (26.79%) in layer 6 (see fig X c).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/summary.png" class="img-fluid figure-img" width="574"></p>
<figcaption><strong>Figure XXX. Region of interest and recording site locations. a)</strong> Coronal slice of the Allen Brain Atlas, highlighting the layers of the primary visual cortex (V1) with distinct colors: yellow for layer 1, light blue for layers 2/3, red for layer 4, blue for layer 5, and green for layer 6. The image base is extracted from the Allen Brain Atlas (<a href="https://atlas.internationalbrainlab.org" class="uri">https://atlas.internationalbrainlab.org</a>) at an Anterior-Posterior (AP) coordinate of -3140 µm <!--# -3140 what? µm ? -->. <strong>b)</strong> Coronal slice from the Allen Brain Atlas that shows an example of a probe insertion site in a mouse brain (subject name: NYU-12). The black line represents the probe path, starting in V1 and ending in the midbrain reticular nucleus (approximately). The image is taken from the IBL online data visualization tool (<a href="https://viz.internationalbrainlab.org" class="uri">https://viz.internationalbrainlab.org</a>). <strong>c)</strong> Pie chart illustrating the proportional distribution of each V1 layer, using the same color scheme as in panel (a). <strong>d)</strong> Scatter plot showing the number of trials (range: 276-1098) and channels (range: 2-118) for the included sessions. The mean number of channels (54.83)<!--# would be good to indicate the range too: you can write (54.83; range: 40-60) for example--> is indicated by a dashed blue vertical line, and the mean number of trials (532.66) is represented by a dashed red horizontal line.</figcaption>
</figure>
</div>
</section>
<section id="behavioral-results" class="level2">
<h2 class="anchored" data-anchor-id="behavioral-results">Behavioral results <!--# you have several results, hence the "s" --></h2>
<p>In line with previous results on whole sessions, mice performed correctly on 80.7% ± 5.8% (mean ± s.d.) of the trials with reaction time (RT) of 1.73 ± 5.7 seconds (mean ± s.d.). RT is defined as the time interval between stimulus onset and when wheel rotation reach threshold of 35° ; and performance is computed as a percent of correct trials over total number of trials. As illustrated in (fig X a;b), Performance improved and reaction times decreased on trials with higher stimulus contrast. In 0% contrast trials, where mice had to rely only on their expectation and prior experience, they made correct choices in 57% ± 8% (mean ± s.d.).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/behavioral.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure XXX. Behavior results.</strong> <strong>a)</strong> illustration of reaction time for different stimulus contrast level using boxplot. <strong>b)</strong> Illustration of performance (as a percent of correct trials over total number of trials) for each contrast levels using boxplots. <!--# "illustration" does not mean much... you must say whether it is mean or median, indicate the number of animals included (N=XXX), and you must specify the meaning of error bars and boxes (SD, SEM, CI, quantiles?). --></figcaption>
</figure>
</div>
</section>
<section id="inter-trial-phase-coherence-itc-1" class="level2">
<h2 class="anchored" data-anchor-id="inter-trial-phase-coherence-itc-1">Inter trial phase coherence (ITC)</h2>
<p>The ITC analysis indicated significant phase alignment in the low-frequency range (2-8 Hz) within the [0, 0.5] second interval following the stimulus (see Fig. X a). To ensure that these findings were not due to chance and to correct for multiple comparisons, we applied the MNE one-sample cluster permutation test (refer to the Statistical Analysis section of the Methods for more details <!--# for now it is not present. -->). The significant clusters, marked by the black line in Fig. X a <!--# I suggest you use the following format "in Figure Xy" (no space between number and panel letter + Figure unnabreviated) -->, demonstrate that the low-frequency ITC during the 0-0.5 second period was statistically meaningful, with a p-value of 0.001. Additionally, as illustrated in Fig. X c, there were no significant differences in ITC across the V1 layers in the low-frequency range.</p>
<p>To assess whether the observed ITC levels were influenced by the stimulus, ITC average levels were compared for each level of stimulus contrast. The average ITC was computed for the low-frequency range (2-8 Hz) and within the 0-0.5 second time window post-stimulus. As illustrated in Fig. X b, an increase in stimulus contrast generally resulted in a higher mean ITC. Interestingly, the only group of trials that did not support this trend was trials without stimulus (i.e.&nbsp;contrast 0%), which will be discussed in the next section.</p>
<p>To statistically evaluate whether the mean ITC was significantly affected by contrast levels, further analysis was undertaken. Given that the Shapiro-Wilk normality test did not confirm normality in the data distribution, the Friedman test, a non-parametric alternative to repeated measures ANOVA, was employed. The results of the Friedman test indicated a highly significant effect of contrast level on ITC mean, with a test statistic of 77.98 and a p-value of <!--# BEWARE: 4.66e-16 is not the usual way of reporting exponents of 10! use instead the following notation: 4.66*10^-16^ that will render well in Quarto--> 4.66*10<sup>-16</sup>, indicating that variations in ITC across different contrast levels were unlikely to have occurred by chance. Due to the significant effect of contrast level on ITC mean identified by the Friedman test, a post-hoc Nemenyi test was performed to determine which specific contrast levels contributed to the observed differences. The Nemenyi test was chosen as it is appropriate for pairwise comparisons following a Friedman test. The post-hoc analysis results are presented in fig <u>X d,</u> with p-values indicating the significance of differences between each pair of contrast levels.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/ITC.png" class="img-fluid figure-img"></p>
<figcaption><strong>Figure XXX. Inter trial phase coherence (ITC).</strong> <strong>a)</strong> ITC average across all subjects and cortical layers relative to stimulus onset. Significant clusters (p = 0.001) are indicated by black lines, as determined by the MNE one-sample cluster permutation test.<strong>b)</strong> Comparison of ITC averages across different stimulus contrast levels using a box plot. The ITC average was computed for the 2-8 Hz frequency range within the 0-0.5 second time window post-stimulus, aggregated across all subjects and layers. <strong>c)</strong> Low-frequency ITC averages for each V1 layer relative to stimulus onset. The layer-specific averages are depicted with solid lines, and their 95% confidence intervals are shaded around the lines in distinct colors: yellow for layer 1, light blue for layers 2/3, red for layer 4, blue for layer 5, and green for layer 6. <strong>d)</strong> Nemenyi post-hoc test p-value results for each pair of contrast levels. Lower p-values indicate significant differences between the ITC average distributions for each pair, represented by a heatmap ranging from yellow (low p-values) to black (high p-values). <!--# It might be more logical to have the diagonal of the panel d in black. Or to reverse the color axis, so that darkest is closer from one. --></figcaption>
</figure>
</div>
</section>
<section id="time-frequency-analysis-results" class="level2">
<h2 class="anchored" data-anchor-id="time-frequency-analysis-results">Time frequency analysis results</h2>
<p>Although there was substantial variability across subjects, the time-frequency analysis of V1 revealed two notable oscillations in relation to the visual stimulus: first, an increase in high-frequency power within the gamma band range (20–40 Hz), and second, a concurrent decrease in lower-frequency power within the 2–7 Hz range. The gamma increase was more transient, while the lower frequency inhibition persisted for a longer duration (see Figure X). As shown in Figure Xa,b, the observed frequency band changes exhibited a similar pattern across the different layers of V1. Statistical tests were not applied to quantitatively evaluate the layer-specificity of these effects, as the variability in the number of channels across layers and subjects did not allow for such an analysis.</p>
<p>The averaged power of each frequency band over the first 1 second after the stimulus was compared for trials with different contrast levels. The Friedman test revealed a statistically significant effect of contrast level on power modulation in both the low-frequency band (test statistic: 20.54, p-value: 0.00039) and the gamma band (test statistic: 18.88, p-value: 0.00083), indicating that power in these bands was significantly influenced by the stimulus contrast. However, as you can see in Figure Xa,b, the frequency band average power across different contrast levels is not quite observable. Additionally, the post-hoc test results shown in Figure Xc,d indicate that the comparisons were mainly significant only in comparison to the 100% contrast condition.</p>
<p>This not readily observable difference is believed to be mainly due to inter-subject variability, with some subjects showing significant contrast-related modulations, while others do not. In general, especially in sessions with high effects of contrast on power bands, higher contrast stimuli involved greater increases in gamma and decreases in theta band power.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/TF.png" class="img-fluid figure-img" width="701"></p>
<figcaption><strong>Average time frequency representation.</strong> Time-frequency is averaged over all channels in the primary visual cortex, computed using the multitaper method. The data is baseline-corrected using the -0.7 to -0.5 s pre-stimulus interval, with time relative to stimulus onset (marked by dashed black vertical line). Power is shown in percent units with a blue-to-warm colormap.</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/layer_power.png" class="img-fluid figure-img"></p>
<figcaption><strong>Average frequency band power across time for all layers of V1</strong>. <strong>a)</strong> low frequency (2-7 Hz) average power for each V1 layer relative to stimulus onset (marked by dashed black vertical line). The layer-specific averages are depicted with solid lines, and their 95% confidence intervals are shaded around the lines in distinct colors: yellow for layer 1, light blue for layers 2/3, red for layer 4, blue for layer 5, and green for layer 6. <strong>b)</strong> similar to panel (a) but for higher frequency band (20-40 Hz)</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/contrast_power.png" class="img-fluid figure-img" width="677"></p>
<figcaption>Affects of stimulus contrast on low and high frequency average power. <strong>a)</strong> The box plots represents the distribution of low-frequency (2-7 Hz) average power across different contrast levels.The power is averaged over the first 1 second after stimulus across all channels of each subject (N= 41). The central line in each box indicates the median power average value, while the box edges represent the inter-quartile range (IQR) and whiskers extend to 1.5 times the IQR. <strong>B)</strong> similar to panel (b) but for high frequency range (20-40 hz). <strong>C)</strong> Nemenyi post-hoc test p-value results comparing low frequency average power for pairwise contrasts levels. Lower p-values indicate significant differences between the low frequency average power distributions for each pair, represented by a heatmap ranging from yellow (low p-values) to black (high p-values). <strong>d)</strong> similar to panel (c) but for high frequency (20-40 Hz) average power</figcaption>
</figure>
</div>
</section>
<section id="phase-amplitude-coupling-pac" class="level2">
<h2 class="anchored" data-anchor-id="phase-amplitude-coupling-pac">Phase amplitude coupling (PAC)</h2>
<p>Phase-amplitude coupling (PAC) analysis revealed that the phase of low-frequency oscillations (2-7 Hz) modulates the amplitude of high-frequency oscillations (25-80 Hz). In Figure X, an example from a single channel illustrates how the amplitude of high-frequency oscillations changes depending on the phase of low-frequency oscillations during the period after the stimulus. This example visually demonstrates the coupling effect; however, it is important to note that this figure serves solely as an illustration of the amplitude modulation by phase, and no specific coupling method (e.g., Gaussian copula) was applied to quantify this effect.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/bin_amplitude.png" class="img-fluid figure-img"></p>
<figcaption><strong>Binned Amplitude by Phase for a channel in V1</strong>. This figure illustrates the binned amplitude of high-frequency oscillations (25-80 Hz) as a function of the phase of low-frequency oscillations (2-7 HZ) for four time windows. From left to right : (-1, -0.5), (-0.5, 0), (0, 1), and (1, 2) with time in seconds relative to the stimulus onset. The data were binned into 20 equal-sized phase bins, and the amplitude was averaged within each bin. The non-uniform distribution of amplitudes indicate high possibility of coupling. As you can see, the time period one second after stimulus (0, 1s) shows observable non-uniform distribution of amplitudes with the highest value for phase zero. This plot is intended to illustrate the relationship between phase and amplitude without applying any coupling quantification. The plot is made using Tensorpac python module <span class="citation" data-cites="combrisson2020">(<a href="#ref-combrisson2020" role="doc-biblioref">Combrisson et al. 2020</a>)</span>.</figcaption>
</figure>
</div>
<p>Comparing PAC values for the time periods before and after the stimulus revealed a significant increase in PAC values following the stimulus. This difference was confirmed by a repeated measures ANOVA, which resulted in F = 5.503, p = 0.0242. The distribution of average PAC values before the stimulus was centered slightly towards negative values (mean: -0.05) and exhibited large tails in both positive and negative directions (standard deviation: 0.05). In contrast, the distribution of PAC values after the stimulus was centered on positive values (mean: 0.02) with narrower tails (standard deviation: 0.01) (see Figure Xa).</p>
<p>In addition, we found that this difference between PAC values were more pronounced in layer five and six (see Figure Xb). As illustrated in Figure X, these two V1 layers were the only layers with F values higher than F-critical (approximately 4). However , similar to Time frequency analysis, no further statistical tests</p>
<p>were applied to quantitatively evaluate the layer-specificity due to the variability in the number of channels across layers.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/pac_beforeVSafter.png" class="img-fluid figure-img"></p>
<figcaption>Phase Amplitude Coupling (PAC) values before and after stimulus. a) Histogram distribution of averaged normalized PAC values for frequency of phase (2-7 HZ) and frequency for amplitude of (25-80 HZ). The distribution is across all V1 channels (N = 2,262) and for two time period: after stimulus (0-1 s) with color yellow and before stimulus (-0.5, 0 s) with color light blue. The histograms are computed with 20 bins, and a kernel density estimate (KDE) is overlaid with solid line. B) Boxplot illustration of before (yellow) and after (light blue) stimulus PAC values for each layer of V1, with number of channels per layer : layer1 = 212, layer 2/3: 338, layer 4 = 650, layer 5 = 650, layer 6 = 606. As you can see the difference is significant mainly in layer 5 and 6.</figcaption>
</figure>
</div>
</section>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<!--# Structure of the discussion: 1. reformulate concisely your main result (one or two finding max) and how they relate to your hypotheses and-or objectives, in this case, you should probably emphasize the "theta as alpha" and the PAC hypotheses 2. restate what you would have ideally liked to see: layer specific activity in different frequency bands, AND why you did not observe exactly that: here, you should mention the noisiness of the data, the type of montage (unipolar montage reduce spatial specificity but CSD increase channel variability, making it difficult to aggregate data, possible effect of channel location3. situate your finding in the literature: say how it is consistent with previous findings, cite the relevant papers, say how it is inconsistent or different from other findings (e.g. less layer specificity as compared to monkeys?) and possibly why (if you choose to discuss the monkey different, maybe say that the columnar organization of the visual cortex is less well established, that vision is less ecologically crucial for rodents..4. describe concisely the obvious next steps: here, it is clearly what we had discussed from day 1: the effect of block expectations on anticipatory neural activity (including theta and PAC, but possibly other metrics too) and neuronal responses to the stimuli depending on whether they appeared on the expected side or not. ALSO: looking in higher order visual areas and even in action related areas, with PAC between low frequency in a distant site related to power high frequency in the primary visual cortex.5. conclude positively -->
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" style="font-size: 75%;" data-entry-spacing="0" role="list">
<div id="ref-aggarwal2022" class="csl-entry" role="listitem">
Aggarwal, Adeeti, Connor Brennan, Jennifer Luo, Helen Chung, Diego Contreras, Max B. Kelz, and Alex Proekt. 2022. <span>“Visual Evoked Feedforward<span></span>feedback Traveling Waves Organize Neural Activity Across the Cortical Hierarchy in Mice.”</span> <em>Nature Communications</em> 13 (1): 4754. <a href="https://doi.org/10.1038/s41467-022-32378-x">https://doi.org/10.1038/s41467-022-32378-x</a>.
</div>
<div id="ref-benson2023" class="csl-entry" role="listitem">
Benson, Brandon, Julius Benson, Daniel Birman, Niccolò Bonacchi, Matteo Carandini, Joana A Catarino, Gaelle A Chapuis, et al. 2023. <span>“A Brain-Wide Map of Neural Activity During Complex Behaviour.”</span> <em>bioRxiv</em>, January, 2023.07.04.547681. <a href="https://doi.org/10.1101/2023.07.04.547681">https://doi.org/10.1101/2023.07.04.547681</a>.
</div>
<div id="ref-bonnefond2015" class="csl-entry" role="listitem">
Bonnefond, Mathilde, and Ole Jensen. 2015. <span>“Gamma Activity Coupled to Alpha Phase as a Mechanism for Top-Down Controlled Gating.”</span> <em>PLOS ONE</em> 10 (6): e0128667. <a href="https://doi.org/10.1371/journal.pone.0128667">https://doi.org/10.1371/journal.pone.0128667</a>.
</div>
<div id="ref-combrisson2020" class="csl-entry" role="listitem">
Combrisson, Etienne, Timothy Nest, Andrea Brovelli, Robin A. A. Ince, Juan L. P. Soto, Aymeric Guillot, and Karim Jerbi. 2020. <span>“Tensorpac: An Open-Source Python Toolbox for Tensor-Based Phase-Amplitude Coupling Measurement in Electrophysiological Brain Signals.”</span> <em>PLOS Computational Biology</em> 16 (10): e1008302. <a href="https://doi.org/10.1371/journal.pcbi.1008302">https://doi.org/10.1371/journal.pcbi.1008302</a>.
</div>
<div id="ref-felleman1991" class="csl-entry" role="listitem">
Felleman, Daniel J, and David C Van Essen. 1991. <span>“Distributed Hierarchical Processing in the Primate Cerebral Cortex.”</span> <em>Cerebral Cortex (New York, NY: 1991)</em> 1 (1): 1–47.
</div>
<div id="ref-huang2011" class="csl-entry" role="listitem">
Huang, Yanping, and Rajesh P. N. Rao. 2011. <span>“Predictive Coding.”</span> <em>WIREs Cognitive Science</em> 2 (5): 580–93. <a href="https://doi.org/10.1002/wcs.142">https://doi.org/10.1002/wcs.142</a>.
</div>
<div id="ref-unified2024" class="csl-entry" role="listitem">
IBL. 2024. <span>“Unified International Brain Laboratory Environment.”</span> <a href="https://github.com/int-brain-lab/iblenv">https://github.com/int-brain-lab/iblenv</a>.
</div>
<div id="ref-jun2017" class="csl-entry" role="listitem">
Jun, James J., Nicholas A. Steinmetz, Joshua H. Siegle, Daniel J. Denman, Marius Bauza, Brian Barbarits, Albert K. Lee, et al. 2017. <span>“Fully Integrated Silicon Probes for High-Density Recording of Neural Activity.”</span> <em>Nature</em> 551 (7679): 232–36. <a href="https://doi.org/10.1038/nature24636">https://doi.org/10.1038/nature24636</a>.
</div>
<div id="ref-nestvogel2022" class="csl-entry" role="listitem">
Nestvogel, Dennis B., and David A. McCormick. 2022. <span>“Visual thalamocortical mechanisms of waking state-dependent activity and alpha oscillations.”</span> <em>Neuron</em> 110 (1): 120–138.e4. <a href="https://doi.org/10.1016/j.neuron.2021.10.005">https://doi.org/10.1016/j.neuron.2021.10.005</a>.
</div>
<div id="ref-senzai2019" class="csl-entry" role="listitem">
Senzai, Yuta, Antonio Fernandez-Ruiz, and György Buzsáki. 2019. <span>“Layer-Specific Physiological Features and Interlaminar Interactions in the Primary Visual Cortex of the Mouse.”</span> <em>Neuron</em> 101: 500–513.
</div>
<div id="ref-vankerkoerle2014" class="csl-entry" role="listitem">
Van Kerkoerle, Timo, Matthew W Self, Bruno Dagnino, Marie-Alice Gariel-Mathis, Jasper Poort, Chris Van Der Togt, and Pieter R Roelfsema. 2014. <span>“Alpha and Gamma Oscillations Characterize Feedback and Feedforward Processing in Monkey Visual Cortex.”</span> <em>Proceedings of the National Academy of Sciences</em> 111 (40): 14332–41.
</div>
<div id="ref-vinck2022" class="csl-entry" role="listitem">
Vinck, Martin, Cem Uran, and Andrés Canales-Johnson. 2022. <span>“The Neural Dynamics of Feedforward and Feedback Interactions in Predictive Processing.”</span> <a href="https://doi.org/10.31234/osf.io/n3afb">https://doi.org/10.31234/osf.io/n3afb</a>.
</div>
<div id="ref-windolf2023" class="csl-entry" role="listitem">
Windolf, Charlie, Han Yu, Angelique C. Paulk, Domokos Meszéna, William Muñoz, Julien Boussard, Richard Hardstone, et al. 2023. <span>“DREDge: Robust Motion Correction for High-Density Extracellular Recordings Across Species.”</span> <em>bioRxiv</em>, October, 2023.10.24.563768. <a href="https://doi.org/10.1101/2023.10.24.563768">https://doi.org/10.1101/2023.10.24.563768</a>.
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    const typesetMath = (el) => {
      if (window.MathJax) {
        // MathJax Typeset
        window.MathJax.typeset([el]);
      } else if (window.katex) {
        // KaTeX Render
        var mathElements = el.getElementsByClassName("math");
        var macros = [];
        for (var i = 0; i < mathElements.length; i++) {
          var texText = mathElements[i].firstChild;
          if (mathElements[i].tagName == "SPAN") {
            window.katex.render(texText.data, mathElements[i], {
              displayMode: mathElements[i].classList.contains('display'),
              throwOnError: false,
              macros: macros,
              fleqn: false
            });
          }
        }
      }
    }
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        for (let i = 0; i < 2; i++) {
          container.appendChild(note.children[i].cloneNode(true));
        }
        typesetMath(container);
        return container.innerHTML
      } else {
        typesetMath(note);
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      typesetMath(note);
      return note.innerHTML;
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>